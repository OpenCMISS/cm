!> \file
!> \author Chris Bradley
!> \brief This module handles all distributed matrix vector routines.
!>
!> \section LICENSE
!>
!> Version: MPL 1.1/GPL 2.0/LGPL 2.1
!>
!> The contents of this file are subject to the Mozilla Public License
!> Version 1.1 (the "License"); you may not use this file except in
!> compliance with the License. You may obtain a copy of the License at
!> http://www.mozilla.org/MPL/
!>
!> Software distributed under the License is distributed on an "AS IS"
!> basis, WITHOUT WARRANTY OF ANY KIND, either express or implied. See the
!> License for the specific language governing rights and limitations
!> under the License.
!>
!> The Original Code is OpenCMISS
!>
!> The Initial Developer of the Original Code is University of Auckland,
!> Auckland, New Zealand, the University of Oxford, Oxford, United
!> Kingdom and King's College, London, United Kingdom. Portions created
!> by the University of Auckland, the University of Oxford and King's
!> College, London are Copyright (C) 2007-2010 by the University of
!> Auckland, the University of Oxford and King's College, London.
!> All Rights Reserved.
!>
!> Contributor(s):
!>
!> Alternatively, the contents of this file may be used under the terms of
!> either the GNU General Public License Version 2 or later (the "GPL"), or
!> the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
!> in which case the provisions of the GPL or the LGPL are applicable instead
!> of those above. If you wish to allow use of your version of this file only
!> under the terms of either the GPL or the LGPL, and not to allow others to
!> use your version of this file under the terms of the MPL, indicate your
!> decision by deleting the provisions above and replace them with the notice
!> and other provisions required by the GPL or the LGPL. If you do not delete
!> the provisions above, a recipient may use your version of this file under
!> the terms of any one of the MPL, the GPL or the LGPL.
!>

!> This module handles all distributed matrix vector routines.
MODULE DISTRIBUTED_MATRIX_VECTOR

  USE BASE_ROUTINES
  USE CMISS_MPI
  USE CMISS_PETSC
  USE COMP_ENVIRONMENT
  USE INPUT_OUTPUT
  USE ISO_VARYING_STRING
  USE ISO_C_BINDING
  USE KINDS
  USE MATRIX_VECTOR
  USE MPI
  USE STRINGS
  USE TYPES
  USE LINKEDLIST_ROUTINES

  IMPLICIT NONE

  PRIVATE

#include "include/petscversion.h"
  
  !Module parameters

  !> \addtogroup DISTRIBUTED_MATRIX_VECTOR_LibraryTypes DISTRIBUTED_MATRIX_VECTOR::LibraryTypes
  !> \brief Distributed matrix-vector library types
  !> \see DISTRIBUTED_MATRIX_VECTOR
  !>@{
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE=LIBRARY_CMISS_TYPE !<CMISS distributed matrix-vector library type \see DISTRIBUTED_MATRIX_VECTOR_LibraryTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE=LIBRARY_PETSC_TYPE !<PETSc distributed matrix-vector library type \see DISTRIBUTED_MATRIX_VECTOR_LibraryTypes,DISTRIBUTED_MATRIX_VECTOR
  !>@}
  
  !> \addtogroup DISTRIBUTED_MATRIX_VECTOR_DataTypes DISTRIBUTED_MATRIX_VECTOR::DataTypes
  !> \brief Distributed matrix-vector data types
  !> \see DISTRIBUTED_MATRIX_VECTOR
  !>@{
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE=MATRIX_VECTOR_INTG_TYPE !<Integer distributed matrix-vector data type \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_SP_TYPE=MATRIX_VECTOR_SP_TYPE !<Single precision real distributed matrix-vector data type \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_DP_TYPE=MATRIX_VECTOR_DP_TYPE !<Double precision real distributed matrix-vector data type \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_L_TYPE=MATRIX_VECTOR_L_TYPE !<Logical distributed matrix-vector data type \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,DISTRIBUTED_MATRIX_VECTOR
  !>@}

  !> \addtogroup DISTRIBUTED_MATRIX_VECTOR_StorageTypes DISTRIBUTED_MATRIX_VECTOR::StorageTypes
  !> \brief Distributed matrix-vector storage type parameters
  !>@{
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE=MATRIX_BLOCK_STORAGE_TYPE !<Distributed matrix block storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE=MATRIX_DIAGONAL_STORAGE_TYPE !<Distributed matrix diagonal storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE=MATRIX_COLUMN_MAJOR_STORAGE_TYPE !<Distributed matrix column major storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE=MATRIX_ROW_MAJOR_STORAGE_TYPE !<Distributed matrix row major storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE=MATRIX_COMPRESSED_ROW_STORAGE_TYPE !<Distributed matrix compressed row storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE=MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE !<Distributed matrix compressed column storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE=MATRIX_ROW_COLUMN_STORAGE_TYPE !<Distributed matrix row-column storage type \see DISTRIBUTED_MATRIX_VECTOR_StorageTypes,MATRIX_VECTOR
  !>@}
  
  !> \addtogroup DISTRIBUTED_MATRIX_VECTOR_GhostingTypes DISTRIBUTED_MATRIX_VECTOR::GhostingTypes
  !> \brief Distributed matrix-vector ghosting types
  !> \see DISTRIBUTED_MATRIX_VECTOR
  !>@{
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE=1 !<Include ghost values in the distributed matrix/vector \see DISTRIBUTED_MATRIX_VECTOR_GhostingTypes,DISTRIBUTED_MATRIX_VECTOR
  INTEGER(INTG), PARAMETER :: DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE=2 !<Do not include ghost values/rows in the distributed matrix/vector \see DISTRIBUTED_MATRIX_VECTOR_GhostingTypes,DISTRIBUTED_MATRIX_VECTOR
  !>@}
  
  !Module types

  !Module variables

  INTEGER(INTG), SAVE :: DISTRIBUTED_DATA_ID=100000000

  !Interfaces

  INTERFACE DISTRIBUTED_MATRIX_ALL_VALUES_SET
    MODULE PROCEDURE DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_ALL_VALUES_SET_L
  END INTERFACE !DISTRIBUTED_MATRIX_ALL_VALUES_SET

  INTERFACE DISTRIBUTED_MATRIX_DATA_GET
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_GET_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_GET_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_GET_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_GET_L
  END INTERFACE !DISTRIBUTED_MATRIX_DATA_GET

  INTERFACE DISTRIBUTED_MATRIX_DATA_RESTORE
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_RESTORE_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_RESTORE_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_RESTORE_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_DATA_RESTORE_L
  END INTERFACE !DISTRIBUTED_MATRIX_DATA_RESTORE

  INTERFACE DISTRIBUTED_MATRIX_VALUES_ADD
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_INTG1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_INTG2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_SP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_SP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_DP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_DP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_L
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_L1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_ADD_L2
  END INTERFACE !DISTRIBUTED_MATRIX_VALUES_ADD

  INTERFACE DISTRIBUTED_MATRIX_VALUES_GET
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_INTG1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_INTG2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_SP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_SP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_DP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_DP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_L
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_L1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_GET_L2
  END INTERFACE !DISTRIBUTED_MATRIX_VALUES_GET

  INTERFACE DISTRIBUTED_MATRIX_VALUES_SET
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_INTG
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_INTG1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_INTG2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_SP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_SP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_SP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_DP
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_DP1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_DP2
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_L
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_L1
    MODULE PROCEDURE DISTRIBUTED_MATRIX_VALUES_SET_L2
  END INTERFACE !DISTRIBUTED_MATRIX_VALUES_SET

   INTERFACE DISTRIBUTED_VECTOR_ALL_VALUES_SET
    MODULE PROCEDURE DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_ALL_VALUES_SET_L
  END INTERFACE !DISTRIBUTED_VECTOR_ALL_VALUES_SET

  INTERFACE DISTRIBUTED_VECTOR_COPY
    MODULE PROCEDURE DISTRIBUTED_VECTOR_COPY_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_COPY_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_COPY_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_COPY_L
  END INTERFACE !DISTRIBUTED_VECTOR_COPY
  
  INTERFACE DISTRIBUTED_VECTOR_DATA_GET
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_GET_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_GET_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_GET_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_GET_L
  END INTERFACE !DISTRIBUTED_VECTOR_DATA_GET

  INTERFACE DISTRIBUTED_VECTOR_DATA_RESTORE
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_RESTORE_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_RESTORE_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_RESTORE_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_DATA_RESTORE_L
  END INTERFACE !DISTRIBUTED_VECTOR_DATA_RESTORE

  INTERFACE DISTRIBUTED_VECTOR_VALUES_ADD
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_INTG1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_SP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_DP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_L
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_ADD_L1
  END INTERFACE !DISTRIBUTED_VECTOR_VALUES_ADD

  INTERFACE DISTRIBUTED_VECTOR_VALUES_GET
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_INTG1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_SP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_DP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_L
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_GET_L1
  END INTERFACE !DISTRIBUTED_VECTOR_VALUES_GET

  INTERFACE DISTRIBUTED_VECTOR_VALUES_SET
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_INTG
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_INTG1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_SP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_SP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_DP
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_DP1
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_L
    MODULE PROCEDURE DISTRIBUTED_VECTOR_VALUES_SET_L1
  END INTERFACE !DISTRIBUTED_VECTOR_VALUES_SET

  INTERFACE DistributedVector_VecDot
    MODULE PROCEDURE DistributedVector_VecDotIntg
    MODULE PROCEDURE DistributedVector_VecDotSp
    MODULE PROCEDURE DistributedVector_VecDotDp
  END INTERFACE !DistributedVector_VecDot

  PUBLIC DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE,DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE

  PUBLIC DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE,DISTRIBUTED_MATRIX_VECTOR_SP_TYPE,DISTRIBUTED_MATRIX_VECTOR_DP_TYPE, &
    & DISTRIBUTED_MATRIX_VECTOR_L_TYPE

  PUBLIC DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE,DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE, &
    & DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE,DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE, &
    & DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE,DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE, &
    & DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE

  PUBLIC DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE,DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE
  
  PUBLIC DISTRIBUTED_MATRIX_ALL_VALUES_SET

  PUBLIC DISTRIBUTED_MATRIX_CREATE_FINISH,DISTRIBUTED_MATRIX_CREATE_START

  PUBLIC DISTRIBUTED_MATRIX_DATA_GET,DISTRIBUTED_MATRIX_DATA_RESTORE

  PUBLIC DistributedMatrix_DataTypeGet, DISTRIBUTED_MATRIX_DATA_TYPE_SET

  PUBLIC DistributedMatrix_DimensionsGet

  PUBLIC DISTRIBUTED_MATRIX_DESTROY

  PUBLIC DISTRIBUTED_MATRIX_DUPLICATE

  PUBLIC DISTRIBUTED_MATRIX_FORM

  PUBLIC DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET

  PUBLIC DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET

  PUBLIC DISTRIBUTED_MATRIX_LINKLIST_SET,DISTRIBUTED_MATRIX_LINKLIST_GET

  PUBLIC DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET

  PUBLIC DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET,DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET

  PUBLIC DISTRIBUTED_MATRIX_OUTPUT

  PUBLIC DISTRIBUTED_MATRIX_OVERRIDE_SET_ON,DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF

  PUBLIC DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET,DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET

  PUBLIC DISTRIBUTED_MATRIX_STORAGE_TYPE_GET,DISTRIBUTED_MATRIX_STORAGE_TYPE_SET

  PUBLIC DISTRIBUTED_MATRIX_UPDATE_START,DISTRIBUTED_MATRIX_UPDATE_FINISH

  PUBLIC DISTRIBUTED_MATRIX_UPDATE_ISFINISHED,DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED

  PUBLIC DISTRIBUTED_MATRIX_VALUES_ADD

  PUBLIC DISTRIBUTED_MATRIX_VALUES_GET,DISTRIBUTED_MATRIX_VALUES_SET

  PUBLIC DISTRIBUTED_MATRIX_BY_VECTOR_ADD
  
  PUBLIC DISTRIBUTED_VECTOR_ALL_VALUES_SET

  PUBLIC DISTRIBUTED_VECTOR_COPY

  PUBLIC DISTRIBUTED_VECTOR_CREATE_FINISH,DISTRIBUTED_VECTOR_CREATE_START

  PUBLIC DISTRIBUTED_VECTOR_DATA_GET,DISTRIBUTED_VECTOR_DATA_RESTORE

  PUBLIC DistributedVector_DataTypeGet, DISTRIBUTED_VECTOR_DATA_TYPE_SET

  PUBLIC DISTRIBUTED_VECTOR_DESTROY

  PUBLIC DISTRIBUTED_VECTOR_DUPLICATE

  PUBLIC DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET

  PUBLIC DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET

  PUBLIC DISTRIBUTED_VECTOR_OUTPUT

  PUBLIC DISTRIBUTED_VECTOR_OVERRIDE_SET_ON,DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF

  PUBLIC DISTRIBUTED_VECTOR_UPDATE_START,DISTRIBUTED_VECTOR_UPDATE_FINISH

  PUBLIC DISTRIBUTED_VECTOR_UPDATE_ISFINISHED,DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED
  
  PUBLIC DISTRIBUTED_VECTOR_VALUES_ADD

  PUBLIC DistributedVector_L2Norm,DistributedVector_VecDot

  PUBLIC DISTRIBUTED_VECTOR_VALUES_GET,DISTRIBUTED_VECTOR_VALUES_SET

CONTAINS  
  
  !
  !================================================================================================================================
  !

  !>Sets all values in an integer distributed matrix to the specified value.
  SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG(DISTRIBUTED_MATRIX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value to set 
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_ALL_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set all values for an integer PETSc distributed matrix.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_INTG

  !
  !================================================================================================================================
  !

  !>Sets all values in a single precision distributed matrix to the specified value.
  SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP(DISTRIBUTED_MATRIX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(SP), INTENT(IN) :: VALUE !<The value to set 
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_ALL_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set all values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_SP

  !
  !================================================================================================================================
  !

  !>Sets all values in a double precision distributed matrix to the specified value.
  SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP(DISTRIBUTED_MATRIX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(DP), INTENT(IN) :: VALUE !<The value to set 
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_ALL_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix cmiss is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(ABS(VALUE)<=ZERO_TOLERANCE) THEN
              IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                CALL PETSC_MATZEROENTRIES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,ERR,ERROR,*999)
              ELSE
                CALL PETSC_MATZEROENTRIES(DISTRIBUTED_MATRIX%PETSC%MATRIX,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix petsc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_DP

  !
  !================================================================================================================================
  !

  !>Sets all values in a logical distributed matrix to the specified value.
  SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_L(DISTRIBUTED_MATRIX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    LOGICAL, INTENT(IN) :: VALUE !<The value to set 
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_ALL_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set all values for a logical PETSc distributed matrix.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_ALL_VALUES_SET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_ALL_VALUES_SET_L

  !
  !================================================================================================================================
  !

  !>Finishes the creation of a CMISS distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH(CMISS_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_CMISS_TYPE), POINTER :: CMISS_MATRIX !<A pointer to the distributed CMISS matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: DOMAIN_MAPPING
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(CMISS_MATRIX)) THEN
      CMISS_MATRIX%BASE_TAG_NUMBER=DISTRIBUTED_DATA_ID
      DOMAIN_MAPPING=>CMISS_MATRIX%DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
      IF(ASSOCIATED(DOMAIN_MAPPING)) THEN
        IF(DOMAIN_MAPPING%NUMBER_OF_DOMAINS==1) THEN
          DISTRIBUTED_DATA_ID=DISTRIBUTED_DATA_ID+1
        ELSE
          DISTRIBUTED_DATA_ID=DISTRIBUTED_DATA_ID+ &
            & DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(DOMAIN_MAPPING%NUMBER_OF_DOMAINS)
        END IF
        CALL MATRIX_CREATE_FINISH(CMISS_MATRIX%MATRIX,ERR,ERROR,*999)
      ELSE
        CALL FLAG_ERROR("Distributed matrix row domain mapping is not associated.",ERR,ERROR,*998)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_CMISS_FINALISE(CMISS_MATRIX,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Finalise a CMISS distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_CMISS_FINALISE(CMISS_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_CMISS_TYPE), POINTER :: CMISS_MATRIX !<A pointer to the CMISS distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_CMISS_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(CMISS_MATRIX)) THEN
      CALL MATRIX_DESTROY(CMISS_MATRIX%MATRIX,ERR,ERROR,*999)
      DEALLOCATE(CMISS_MATRIX)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_FINALSE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_CMISS_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_CMISS_FINALISE

  !
  !================================================================================================================================
  !

  !>Intialises a CMISS distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_CMISS_INITIALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_CMISS_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
        CALL FLAG_ERROR("CMISS is already associated for this distributed matrix.",ERR,ERROR,*998)
      ELSE
        ROW_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
        COLUMN_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
        IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
          IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
            ALLOCATE(DISTRIBUTED_MATRIX%CMISS,STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed matrix.",ERR,ERROR,*999)
            DISTRIBUTED_MATRIX%CMISS%DISTRIBUTED_MATRIX=>DISTRIBUTED_MATRIX
            DISTRIBUTED_MATRIX%LIBRARY_TYPE=DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE
            NULLIFY(DISTRIBUTED_MATRIX%CMISS%MATRIX)
            !Set the defaults
            CALL MATRIX_CREATE_START(DISTRIBUTED_MATRIX%CMISS%MATRIX,ERR,ERROR,*999)
            CALL MATRIX_DATA_TYPE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,MATRIX_VECTOR_DP_TYPE,ERR,ERROR,*999)
            CALL MATRIX_STORAGE_TYPE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,MATRIX_BLOCK_STORAGE_TYPE,ERR,ERROR,*999)
            SELECT CASE(DISTRIBUTED_MATRIX%GHOSTING_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
              CALL MATRIX_SIZE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL, &
                & COLUMN_DOMAIN_MAPPING%NUMBER_OF_GLOBAL,ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
              CALL MATRIX_SIZE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_DOMAIN_MAPPING%NUMBER_OF_LOCAL, &
                & COLUMN_DOMAIN_MAPPING%NUMBER_OF_GLOBAL,ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The distributed matrix ghosting type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("Distributed matrix column domain mapping is not associated.",ERR,ERROR,*998)
          ENDIF
        ELSE
          CALL FLAG_ERROR("Distributed matrix row domain mapping is not associated.",ERR,ERROR,*998)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_INITIALSE")
#endif
    RETURN
999 IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) &
      & CALL DISTRIBUTED_MATRIX_CMISS_FINALISE(DISTRIBUTED_MATRIX%CMISS,DUMMY_ERR,DUMMY_ERROR,*999)
998 CALL ERRORS("DISTRIBUTED_MATRIX_CMISS_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CMISS_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_CMISS_INITIALISE

  !
  !================================================================================================================================
  !

  !>Finishes the creation of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_CREATE_FINISH(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables 
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_CREATE_FINISH",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*998)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CALL DISTRIBUTED_MATRIX_CMISS_CREATE_FINISH(DISTRIBUTED_MATRIX%CMISS,ERR,ERROR,*999)
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH(DISTRIBUTED_MATRIX%PETSC,ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT        
        DISTRIBUTED_MATRIX%MATRIX_FINISHED=.TRUE.
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CREATE_FINISH")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_FINALISE(DISTRIBUTED_MATRIX,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Starts the creation of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_CREATE_START(ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING,DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING !<A pointer to the row domain mapping to be used for the distribution
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: COLUMN_DOMAIN_MAPPING !<A pointer to the column domain mapping to be used for the distribution
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<On return a pointer to the distributed matrix being created
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_CREATE_START",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
      IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
        IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
          CALL FLAG_ERROR("Distributed matrix is already associated.",ERR,ERROR,*998)
        ELSE
          IF(ROW_DOMAIN_MAPPING%NUMBER_OF_DOMAINS==COLUMN_DOMAIN_MAPPING%NUMBER_OF_DOMAINS) THEN
            CALL DISTRIBUTED_MATRIX_INITIALISE(ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING,DISTRIBUTED_MATRIX,ERR,ERROR,*999)
            !Set the defaults
          ELSE
            LOCAL_ERROR="The number of domains in the row domain mapping ("// &
              & TRIM(NUMBER_TO_VSTRING(ROW_DOMAIN_MAPPING%NUMBER_OF_DOMAINS,"*",ERR,ERROR))// &
              & ") does not match the number of domains in the column domain mapping ("// &
              & TRIM(NUMBER_TO_VSTRING(COLUMN_DOMAIN_MAPPING%NUMBER_OF_DOMAINS,"*",ERR,ERROR))//")."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ENDIF
      ELSE
        CALL FLAG_ERROR("Column domain mapping is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Row domain mapping is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CREATE_START")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_FINALISE(DISTRIBUTED_MATRIX,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_CREATE_START",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_CREATE_START")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_CREATE_START

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of an integer distributed matrix. Note: the values can be used for read operations but a DISTRIBUTED_MATRIX_VALUES_SET call must be used to change any values. The pointer should not be deallocated and a DISTRIBUTED_MATRIX_DATA_RESTORE call must be used after the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_INTG(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), POINTER :: DATA(:) !<On return a pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_GET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
              CALL MATRIX_DATA_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,DATA,ERR,ERROR,*999)
            ELSE
              CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get data for an integer PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_GET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_INTG

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of a single precision distributed matrix. Note: the values can be used for read operations but a DISTRIBUTED_MATRIX_VALUES_SET call must be used to change any values. The pointer should not be deallocated and a DISTRIBUTED_MATRIX_DATA_RESTORE call must be used after the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_SP(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(SP), POINTER :: DATA(:) !<On return a pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_GET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
              CALL MATRIX_DATA_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,DATA,ERR,ERROR,*999)
            ELSE
              CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get data for a single precision PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_GET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_SP

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of a double precision distributed matrix. Note: the values can be used for read operations but a DISTRIBUTED_MATRIX_VALUES_SET call must be used to change any values. The pointer should not be deallocated and a DISTRIBUTED_MATRIX_DATA_RESTORE call must be used after the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_DP(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(DP), POINTER :: DATA(:) !<On return a pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    REAL(DP), POINTER :: petscData(:,:)
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_GET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
              CALL MATRIX_DATA_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,DATA,ERR,ERROR,*999)
            ELSE
              CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
#if ( PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 4 )
              IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
                CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                  CALL PETSC_MATDENSEGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                  CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                  CALL PETSC_MATSEQAIJGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                    & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ELSE
                SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
                CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                  CALL PETSC_MATDENSEGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                  CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                  CALL PETSC_MATSEQAIJGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                    & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ENDIF
#else
              IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                CALL PETSC_MATGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData,ERR,ERROR,*999)
              ELSE
                CALL PETSC_MATGETARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData,ERR,ERROR,*999)
              ENDIF
#endif
              ! Convert 2D array from PETSc to 1D array
              ! Using C_F_POINTER(C_LOC(... is a bit ugly but transfer doesn't work with pointers
              SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
              CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                CALL C_F_POINTER(C_LOC(petscData(1,1)),DATA,[DISTRIBUTED_MATRIX%PETSC%M*DISTRIBUTED_MATRIX%PETSC%N])
              CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                !PETSc returns an m * n matrix rather than number non-zeros by 1, so the returned
                !2D array actually contains junk data outside of the actual matrix.
                !This is a bug in PETSc but we can get the correct 1D data here
                CALL C_F_POINTER(C_LOC(petscData(1,1)),DATA,[DISTRIBUTED_MATRIX%PETSC%NUMBER_NON_ZEROS])
              CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE DEFAULT
                LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                  & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              END SELECT
            ELSE
              CALL FLAG_ERROR("Distributed matris PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_GET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_DP

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of a logical distributed matrix. Note: the values can be used for read operations but a DISTRIBUTED_MATRIX_VALUES_SET call must be used to change any values. The pointer should not be deallocated and a DISTRIBUTED_MATRIX_DATA_RESTORE call must be used after the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_L(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    LOGICAL, POINTER :: DATA(:) !<On return a pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
 
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_GET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
              CALL MATRIX_DATA_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,DATA,ERR,ERROR,*999)
            ELSE
              CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get data for a logical PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_GET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_GET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_GET_L

  !
  !================================================================================================================================
  !

  !>Restores the integer data pointer returned from DISTRIBUTED_MATRIX_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_INTG(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), POINTER :: DATA(:) !<The a pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_RESTORE_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)              
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for an integer PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_RESTORE_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_INTG

  !
  !================================================================================================================================
  !

  !>Restores the single precision data pointer returned from DISTRIBUTED_MATRIX_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_SP(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(SP), POINTER :: DATA(:) !<A pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_RESTORE_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for a single precision PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_RESTORE_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_SP

  !
  !================================================================================================================================
  !

  !>Restores the double precision data pointer returned from DISTRIBUTED_MATRIX_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_DP(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    REAL(DP), POINTER :: DATA(:) !<A pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    REAL(DP), POINTER :: petscData(:,:)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_RESTORE_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
              SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
              CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                !Convert 1D array to 2D
                CALL C_F_POINTER(C_LOC(DATA(1)),petscData,[DISTRIBUTED_MATRIX%PETSC%M,DISTRIBUTED_MATRIX%PETSC%N])
              CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                !PETSc expects an m * n 2D matrix rather than a 1D array with length equal to number of non-zeros
                !This is a bug in PETSc so we have to give it a 2D matrix with junk at the end
                CALL C_F_POINTER(C_LOC(DATA(1)),petscData,[DISTRIBUTED_MATRIX%PETSC%M,DISTRIBUTED_MATRIX%PETSC%N])
              CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
              CASE DEFAULT
                LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                  & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              END SELECT
#if ( PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 4 )
              IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
                CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                  CALL PETSC_MATDENSERESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                  CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                  CALL PETSC_MATSEQAIJRESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                    & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ELSE
                SELECT CASE(DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE)
                CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                  CALL PETSC_MATDENSERESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                  CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                  CALL PETSC_MATSEQAIJRESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING( &
                    & DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ENDIF
#else
              IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                CALL PETSC_MATRESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,petscData, &
                  & ERR,ERROR,*999)
              ELSE
                CALL PETSC_MATRESTOREARRAYF90(DISTRIBUTED_MATRIX%PETSC%MATRIX,petscData, &
                  & ERR,ERROR,*999)
              ENDIF
#endif
            ELSE
              CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_RESTORE_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_DP

  !
  !================================================================================================================================
  !

  !>Restores the logical data pointer returned from DISTRIBUTED_MATRIX_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_L(DISTRIBUTED_MATRIX,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    LOGICAL, POINTER :: DATA(:) !<A pointer to the distributed matrix data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
 
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_RESTORE_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for a logical PETSc distributed matrix.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_RESTORE_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_RESTORE_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_RESTORE_L
  
  !
  !================================================================================================================================
  !

  !>Gets the data type of a distributed matrix.
  SUBROUTINE DistributedMatrix_DataTypeGet(matrix,dataType,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: matrix !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: dataType !<On return, the data type of the matrix. \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string

    CALL enters("DistributedMatrix_DataTypeGet",err,error,*999)

    IF(ASSOCIATED(matrix)) THEN
      IF(.NOT.matrix%matrix_finished) THEN
        CALL flag_error("The matrix has not been finished.",err,error,*999)
      ELSE
        dataType=matrix%data_type
      END IF
    ELSE
      CALL flag_error("Distributed matrix is not associated.",err,error,*999)
    END IF

    CALL exits("DistributedMatrix_DataTypeGet")
    RETURN
999 CALL errors("DistributedMatrix_DataTypeGet",err,error)
    CALL exits("DistributedMatrix_DataTypeGet")
    RETURN 1
  END SUBROUTINE DistributedMatrix_DataTypeGet

  !
  !================================================================================================================================
  !

  !>Sets/changes the data type of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_DATA_TYPE_SET(DISTRIBUTED_MATRIX,DATA_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: DATA_TYPE !<The data type to set. \see MATRIX_VECTOR_DataTypes,MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DATA_TYPE_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_DATA_TYPE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,DATA_TYPE,ERR,ERROR,*999)
            DISTRIBUTED_MATRIX%DATA_TYPE=DATA_TYPE
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          SELECT CASE(DATA_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE)
            CALL FLAG_ERROR("An integer distributed PETSc matrix is not implemented.",ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_SP_TYPE)
            CALL FLAG_ERROR("A single precision distributed PETSc matrix is not implemented.",ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_DP_TYPE)
            DISTRIBUTED_MATRIX%DATA_TYPE=DISTRIBUTED_MATRIX_VECTOR_DP_TYPE
          CASE(DISTRIBUTED_MATRIX_VECTOR_L_TYPE)
            CALL FLAG_ERROR("A logical distributed PETSc matrix is not implemented.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The specified data type of "//TRIM(NUMBER_TO_VSTRING(DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_TYPE_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DATA_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DATA_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DATA_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Gets the dimensions of a matrix on this computational node.
  SUBROUTINE DistributedMatrix_DimensionsGet(distributedMatrix,m,n,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: distributedMatrix !<A pointer to the distributed matrix to get dimensions for
    INTEGER(INTG), INTENT(OUT) :: m !<On return, the number of rows in the matrix for this domain
    INTEGER(INTG), INTENT(OUT) :: n !<On return, the number of columns in the matrix for this domain
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string
    !Local variables
    TYPE(MATRIX_TYPE), POINTER :: matrix
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: petscMatrix
    TYPE(VARYING_STRING) :: localError

    CALL enters("DistributedMatrix_DimensionsGet",err,error,*999)

    IF(ASSOCIATED(distributedMatrix)) THEN
      SELECT CASE(distributedMatrix%library_type)
      CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
        IF(ASSOCIATED(distributedMatrix%cmiss)) THEN
          matrix=>distributedMatrix%cmiss%matrix
          IF(ASSOCIATED(matrix)) THEN
            IF(.NOT.matrix%matrix_finished) THEN
              CALL flag_error("The matrix has not been finished.",err,error,*999)
            ELSE
              m=matrix%m
              n=matrix%n
            END IF
          ELSE
            CALL flag_error("Distributed matrix CMISS matrix is not associated.",err,error,*999)
          END IF
        ELSE
          CALL flag_error("Distributed matrix CMISS is not associated.",err,error,*999)
        END IF
      CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
        petscMatrix=>distributedMatrix%petsc
        IF(ASSOCIATED(petscMatrix)) THEN
          m=petscMatrix%m
          n=petscMatrix%n
        ELSE
          CALL flag_error("Distributed matrix PETSc is not associated.",err,error,*999)
        END IF
      CASE DEFAULT
        localError="The distributed matrix library type of "// &
          & TRIM(number_to_vstring(distributedMatrix%library_type,"*",err,error))//" is invalid."
        CALL flag_error(localError,err,error,*999)
      END SELECT
    ELSE
      CALL flag_error("Distributed matrix is not associated.",err,error,*999)
    END IF

    CALL exits("DistributedMatrix_DimensionsGet")
    RETURN
999 CALL errors("DistributedMatrix_DimensionsGet",err,error)
    CALL exits("DistributedMatrix_DimensionsGet")
    RETURN 1
  END SUBROUTINE DistributedMatrix_DimensionsGet

  !
  !================================================================================================================================
  !

  !>Destroys a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_DESTROY(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DESTROY",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      CALL DISTRIBUTED_MATRIX_FINALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*999)
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DESTROY")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_DESTROY",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DESTROY")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DESTROY

  !
  !================================================================================================================================
  !

  !>Duplicates the structure of a distributed matrix and returns a pointer to the new matrix in NEW_DISTRIBUTED_MATRIX.
  SUBROUTINE DISTRIBUTED_MATRIX_DUPLICATE(DISTRIBUTED_MATRIX,NEW_DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix to duplicate
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: NEW_DISTRIBUTED_MATRIX !<On return a pointer to the new duplicated distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_DUPLICATE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(NEW_DISTRIBUTED_MATRIX)) THEN
        CALL FLAG_ERROR("New distributed matrix is already associated.",ERR,ERROR,*998)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL DISTRIBUTED_MATRIX_CREATE_START(DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING,DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING, &
              & NEW_DISTRIBUTED_MATRIX,ERR,ERROR,*999)
            CALL MATRIX_DUPLICATE(DISTRIBUTED_MATRIX%CMISS%MATRIX,NEW_DISTRIBUTED_MATRIX%CMISS%MATRIX,ERR,ERROR,*999)
            CALL DISTRIBUTED_MATRIX_CREATE_FINISH(NEW_DISTRIBUTED_MATRIX,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF          
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL DISTRIBUTED_MATRIX_CREATE_START(DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING,DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING, &
            & NEW_DISTRIBUTED_MATRIX,ERR,ERROR,*999)
          CALL DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET(NEW_DISTRIBUTED_MATRIX,DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE,ERR,ERROR,*999)
          CALL DISTRIBUTED_MATRIX_CREATE_FINISH(NEW_DISTRIBUTED_MATRIX,ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DUPLICATE")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_FINALISE(NEW_DISTRIBUTED_MATRIX,DUMMY_ERR,DUMMY_ERROR,*999)
998 CALL ERRORS("DISTRIBUTED_MATRIX_DUPLICATE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_DUPLICATE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_DUPLICATE

  !
  !================================================================================================================================
  !

  !>Finalises a distributed matrix and deallocates all memory.
  SUBROUTINE DISTRIBUTED_MATRIX_FINALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      CALL DISTRIBUTED_MATRIX_CMISS_FINALISE(DISTRIBUTED_MATRIX%CMISS,ERR,ERROR,*999)
      CALL DISTRIBUTED_MATRIX_PETSC_FINALISE(DISTRIBUTED_MATRIX%PETSC,ERR,ERROR,*999)        
      DEALLOCATE(DISTRIBUTED_MATRIX)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_FINALISE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_FINALISE

  !
  !================================================================================================================================
  !

  !>Forms a distributed matrix by initialising the structure of the matrix to zero
  SUBROUTINE DISTRIBUTED_MATRIX_FORM(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix to form.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: column_idx,row
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: PETSC_MATRIX
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_FORM",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)          
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          PETSC_MATRIX=>DISTRIBUTED_MATRIX%PETSC
          IF(ASSOCIATED(PETSC_MATRIX)) THEN
            SELECT CASE(PETSC_MATRIX%STORAGE_TYPE)
            CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
              CALL PETSC_MATZEROENTRIES(PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
              CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
              IF(PETSC_MATRIX%USE_OVERRIDE_MATRIX) THEN
                DO row=1,PETSC_MATRIX%M
                  DO column_idx=PETSC_MATRIX%ROW_INDICES(row),PETSC_MATRIX%ROW_INDICES(row+1)-1
                    CALL PETSC_MATSETVALUE(PETSC_MATRIX%OVERRIDE_MATRIX,PETSC_MATRIX%GLOBAL_ROW_NUMBERS(row), &
                      & PETSC_MATRIX%COLUMN_INDICES(column_idx)-1,0.0_DP,PETSC_INSERT_VALUES, &
                      & ERR,ERROR,*999) !PETSc uses 0 indicies
                  ENDDO !column_idx
                ENDDO !row_idx
              ELSE
                DO row=1,PETSC_MATRIX%M
                  DO column_idx=PETSC_MATRIX%ROW_INDICES(row),PETSC_MATRIX%ROW_INDICES(row+1)-1
                    CALL PETSC_MATSETVALUE(PETSC_MATRIX%MATRIX,PETSC_MATRIX%GLOBAL_ROW_NUMBERS(row), &
                      & PETSC_MATRIX%COLUMN_INDICES(column_idx)-1,0.0_DP,PETSC_INSERT_VALUES, &
                      & ERR,ERROR,*999) !PETSc uses 0 indicies
                  ENDDO !column_idx
                ENDDO !row_idx
              ENDIF
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%STORAGE_TYPE,"*",ERR,ERROR))// &
                & " is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
            IF(PETSC_MATRIX%USE_OVERRIDE_MATRIX) THEN
              CALL PETSC_MATASSEMBLYBEGIN(PETSC_MATRIX%OVERRIDE_MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
              CALL PETSC_MATASSEMBLYEND(PETSC_MATRIX%OVERRIDE_MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ELSE
              CALL PETSC_MATASSEMBLYBEGIN(PETSC_MATRIX%MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
              CALL PETSC_MATASSEMBLYEND(PETSC_MATRIX%MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_FORM")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_FORM",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_FORM")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_FORM

  !
  !================================================================================================================================
  !

  !>Sets/changes the ghosting type for a distributed matrix
  SUBROUTINE DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET(DISTRIBUTED_MATRIX,GHOSTING_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: GHOSTING_TYPE !<The ghosting type \see DISTRIBUTED_MATRIX_VECTOR_GhostingTypes,DISTRIBUTED_MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has already been finished.",ERR,ERROR,*999)
      ELSE
        ROW_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
        COLUMN_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
        IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
          IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
            SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
                SELECT CASE(GHOSTING_TYPE)
                CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
                  CALL MATRIX_SIZE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL, &
                    & COLUMN_DOMAIN_MAPPING%NUMBER_OF_GLOBAL,ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
                  CALL MATRIX_SIZE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_DOMAIN_MAPPING%NUMBER_OF_LOCAL, &
                    & COLUMN_DOMAIN_MAPPING%NUMBER_OF_GLOBAL,ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The given ghosting type of "//TRIM(NUMBER_TO_VSTRING(GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ELSE
                CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
                SELECT CASE(GHOSTING_TYPE)
                CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
                  DISTRIBUTED_MATRIX%PETSC%N=ROW_DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
                CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
                  DISTRIBUTED_MATRIX%PETSC%N=ROW_DOMAIN_MAPPING%NUMBER_OF_LOCAL
                CASE DEFAULT
                  LOCAL_ERROR="The given ghosting type of "//TRIM(NUMBER_TO_VSTRING(GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ELSE
                CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed matrix library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
            DISTRIBUTED_MATRIX%GHOSTING_TYPE=GHOSTING_TYPE
          ELSE
            CALL FLAG_ERROR("Distributed matrix column domain mapping is not associated.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("Distributed matrix row domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_GHOSTING_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Sets/changes the library type for a distributed matrix
  SUBROUTINE DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET(DISTRIBUTED_MATRIX,LIBRARY_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix 
    INTEGER(INTG), INTENT(IN) :: LIBRARY_TYPE !<The library type \see DISTRIBUTED_MATRIX_VECTOR_LibraryTypes,DISTRIBUTED_MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR,OLD_LIBRARY_TYPE
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has already been finished.",ERR,ERROR,*998)
      ELSE
        OLD_LIBRARY_TYPE=DISTRIBUTED_MATRIX%LIBRARY_TYPE
        IF(LIBRARY_TYPE/=OLD_LIBRARY_TYPE) THEN
          !Initialise the new library type
          SELECT CASE(LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            CALL DISTRIBUTED_MATRIX_CMISS_INITIALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL DISTRIBUTED_MATRIX_PETSC_INITIALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The library type of "//TRIM(NUMBER_TO_VSTRING(LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          !Finalise the old library type
          SELECT CASE(OLD_LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            CALL DISTRIBUTED_MATRIX_CMISS_FINALISE(DISTRIBUTED_MATRIX%CMISS,ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL DISTRIBUTED_MATRIX_PETSC_FINALISE(DISTRIBUTED_MATRIX%PETSC,ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(OLD_LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT          
          DISTRIBUTED_MATRIX%LIBRARY_TYPE=LIBRARY_TYPE
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET")
#endif
    RETURN
999 SELECT CASE(LIBRARY_TYPE)
    CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
      CALL DISTRIBUTED_MATRIX_CMISS_FINALISE(DISTRIBUTED_MATRIX%CMISS,DUMMY_ERR,DUMMY_ERROR,*998)
    CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
      CALL DISTRIBUTED_MATRIX_PETSC_FINALISE(DISTRIBUTED_MATRIX%PETSC,DUMMY_ERR,DUMMY_ERROR,*998)
    END SELECT
998 CALL ERRORS("DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_LIBRARY_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Intialises a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_INITIALISE(ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING,DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING !<A pointer to the row domain mapping used to distribute this matrix
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: COLUMN_DOMAIN_MAPPING !<A pointer to the column domain mapping used to distribute this matrix
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
      IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
        IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
          CALL FLAG_ERROR("Distributed matrix is already associated.",ERR,ERROR,*998)
        ELSE
          ALLOCATE(DISTRIBUTED_MATRIX,STAT=ERR)
          IF(ERR/=0) CALL FLAG_ERROR("Could not allocated the distributed matrix.",ERR,ERROR,*999)
          DISTRIBUTED_MATRIX%MATRIX_FINISHED=.FALSE.
          DISTRIBUTED_MATRIX%LIBRARY_TYPE=0
          DISTRIBUTED_MATRIX%GHOSTING_TYPE=DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE
          DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING=>ROW_DOMAIN_MAPPING
          DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING=>COLUMN_DOMAIN_MAPPING
          DISTRIBUTED_MATRIX%DATA_TYPE=MATRIX_VECTOR_DP_TYPE
          NULLIFY(DISTRIBUTED_MATRIX%CMISS)
          NULLIFY(DISTRIBUTED_MATRIX%PETSC)
          CALL DISTRIBUTED_MATRIX_CMISS_INITIALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Column domain mapping is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Row domain mapping is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_INITIALSE")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_FINALISE(DISTRIBUTED_MATRIX,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_INITIALISE

  !
  !================================================================================================================================
  !

  !>Gets the maximum number of columns in each row of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET(DISTRIBUTED_MATRIX,MAX_COLUMNS_PER_ROW,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: MAX_COLUMNS_PER_ROW !<On return, the maximum number of columns in each row
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
       SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_MAX_COLUMNS_PER_ROW_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,MAX_COLUMNS_PER_ROW,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            MAX_COLUMNS_PER_ROW=DISTRIBUTED_MATRIX%PETSC%MAXIMUM_COLUMN_INDICES_PER_ROW
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed mtrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_MAX_COLUMNS_PER_ROW_GET

  !
  !================================================================================================================================
  !

  !>Sets/changes the number of non zeros for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET(DISTRIBUTED_MATRIX,NUMBER_NON_ZEROS,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: NUMBER_NON_ZEROS !<The number of non zeros in the matrix to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_NUMBER_NON_ZEROS_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,NUMBER_NON_ZEROS,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(NUMBER_NON_ZEROS>0) THEN
              DISTRIBUTED_MATRIX%PETSC%NUMBER_NON_ZEROS=NUMBER_NON_ZEROS
            ELSE
              LOCAL_ERROR="The specified number of non zeros ("//TRIM(NUMBER_TO_VSTRING(NUMBER_NON_ZEROS,"*",ERR,ERROR))// &
                & ") is invalid. The number must be > 0."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed mtrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_SET

  !
  !================================================================================================================================
  !

  !>Gets the number of non zeros for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET(DISTRIBUTED_MATRIX,NUMBER_NON_ZEROS,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: NUMBER_NON_ZEROS !<On return, the number of non zeros in the matrix to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_NUMBER_NON_ZEROS_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,NUMBER_NON_ZEROS,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            NUMBER_NON_ZEROS=DISTRIBUTED_MATRIX%PETSC%NUMBER_NON_ZEROS
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix is not finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET

  !
  !================================================================================================================================
  !
  !================================================================================================================================
  !

  !>Sets/changes the LIST STRUCTURE for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_LINKLIST_SET(DISTRIBUTED_MATRIX,LIST,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    type(LinkedList),pointer :: list(:) 
    !INTEGER(INTG), INTENT(IN) :: NUMBER_NON_ZEROS !<The number of non zeros in the matrix to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_LINKLIST_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_LINKLIST_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,LIST,ERR,ERROR,*999)
            !DISTRIBUTED_MATRIX%CMISS%list=list 
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            !IF(NUMBER_NON_ZEROS>0) THEN
              ! Check this
              DISTRIBUTED_MATRIX%PETSC%list=>list
            !ELSE
            !  LOCAL_ERROR="The specified number of non zeros ("//TRIM(NUMBER_TO_VSTRING(NUMBER_NON_ZEROS,"*",ERR,ERROR))// &
            !    & ") is invalid. The number must be > 0."
            !  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            !ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed mtrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LIKLIST_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_LINKLIST_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LINKLIST_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_LINKLIST_SET

  !
  !================================================================================================================================
  !

  !>Gets the LINKLIST STURUCTURE for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_LINKLIST_GET(DISTRIBUTED_MATRIX,LIST,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    type(LinkedList),pointer :: list(:) 
!    INTEGER(INTG), INTENT(OUT) :: NUMBER_NON_ZEROS !<On return, the number of non zeros in the matrix to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_NUMBER_NON_ZEROS_GET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            !list=DISTRIBUTED_MATRIX%CMISS%list
            CALL MATRIX_LINKLIST_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,LIST,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            list=>DISTRIBUTED_MATRIX%PETSC%list
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix is not finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LINKLIST_GET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_LINKLIST_GET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_LINKLIST_GET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_LINKLIST_GET

  !
  !================================================================================================================================
  !
  !>Outputs a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_OUTPUT(ID,DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    INTEGER(INTG), INTENT(IN) :: ID !<The ID of the output stream
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
!    INTEGER(INTG) :: i,NUMBER_OF_COLUMNS
!    INTEGER(INTG), ALLOCATABLE :: COLUMNS(:)
!    REAL(DP), ALLOCATABLE :: VALUES(:)
!    CHARACTER(LEN=9) :: ROW_STRING
!    CHARACTER(LEN=39) :: INITIAL_STRING
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_OUTPUT",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_OUTPUT(ID,DISTRIBUTED_MATRIX%CMISS%MATRIX,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN 
              CALL PETSC_MATVIEW(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,PETSC_VIEWER_STDOUT_WORLD,ERR,ERROR,*999) 
            ELSE 
              CALL PETSC_MATVIEW(DISTRIBUTED_MATRIX%PETSC%MATRIX,PETSC_VIEWER_STDOUT_WORLD,ERR,ERROR,*999) 
            ENDIF
            !ALLOCATE(COLUMNS(DISTRIBUTED_MATRIX%PETSC%MAXIMUM_COLUMN_INDICES_PER_ROW),STAT=ERR)
            !IF(ERR/=0) CALL FLAG_ERROR("Could not allocate columns.",ERR,ERROR,*999)
            !ALLOCATE(VALUES(DISTRIBUTED_MATRIX%PETSC%MAXIMUM_COLUMN_INDICES_PER_ROW),STAT=ERR)
            !IF(ERR/=0) CALL FLAG_ERROR("Could not allocate values.",ERR,ERROR,*999)
            !DO i=1,DISTRIBUTED_MATRIX%PETSC%M
            !  IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
            !    CALL PETSC_MATGETROW(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,i-1,NUMBER_OF_COLUMNS,COLUMNS,VALUES, &
            !      & ERR,ERROR,*999)
            !  ELSE
            !    CALL PETSC_MATGETROW(DISTRIBUTED_MATRIX%PETSC%MATRIX,i-1,NUMBER_OF_COLUMNS,COLUMNS,VALUES,ERR,ERROR,*999)
            !  ENDIF
            !  ROW_STRING=NUMBER_TO_CHARACTER(i,"I9",ERR,ERROR)
            !  INITIAL_STRING='("Matrix('//ROW_STRING//',:):",8(X,E13.6))'
            !  CALL WRITE_STRING_VECTOR(ID,1,1,NUMBER_OF_COLUMNS,8,8,VALUES,INITIAL_STRING,'(20X,8(X,E13.6))', &
            !    & ERR,ERROR,*999)
            !  IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
            !    CALL PETSC_MATRESTOREROW(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,i-1,NUMBER_OF_COLUMNS,COLUMNS,VALUES, &
            !      & ERR,ERROR,*999)
            !  ELSE
            !    CALL PETSC_MATRESTOREROW(DISTRIBUTED_MATRIX%PETSC%MATRIX,i-1,NUMBER_OF_COLUMNS,COLUMNS,VALUES,ERR,ERROR,*999)
            !  ENDIF
            !ENDDO !i
            !IF(ALLOCATED(VALUES)) DEALLOCATE(VALUES)
            !IF(ALLOCATED(COLUMNS)) DEALLOCATE(COLUMNS)
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("Distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OUTPUT")
#endif
    RETURN
!999 IF(ALLOCATED(VALUES)) DEALLOCATE(VALUES)
!    IF(ALLOCATED(COLUMNS)) DEALLOCATE(COLUMNS)
999 CALL ERRORS("DISTRIBUTED_MATRIX_OUTPUT",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OUTPUT")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_OUTPUT

  !
  !================================================================================================================================
  !

  !>Sets the override matrix for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_OVERRIDE_SET_ON(DISTRIBUTED_MATRIX,OVERRIDE_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix to override
    TYPE(PETSC_MAT_TYPE), INTENT(IN) :: OVERRIDE_MATRIX !<The override matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_OVERRIDE_SET_ON",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)          
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX=.TRUE.
            DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX=OVERRIDE_MATRIX
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
       CALL FLAG_ERROR("Distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OVERRIDE_SET_ON")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_OVERRIDE_SET_ON",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OVERRIDE_SET_ON")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_OVERRIDE_SET_ON

  !
  !================================================================================================================================
  !

  !>Turns off the override matrix for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix to override
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)          
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX=.FALSE.
            CALL PETSC_MATINITIALISE(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("Distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_OVERRIDE_SET_OFF

  !
  !================================================================================================================================
  !

  !>Finishes the creation of a CMISS distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH(PETSC_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: PETSC_MATRIX !<A pointer to the distributed PETSc matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR,i
    !INTEGER(INTG), ALLOCATABLE :: GLOBAL_NUMBERS(:)
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(PETSC_MATRIX)) THEN
      DISTRIBUTED_MATRIX=>PETSC_MATRIX%DISTRIBUTED_MATRIX
      IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
        ROW_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
        COLUMN_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
        IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
          IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
            SELECT CASE(PETSC_MATRIX%STORAGE_TYPE)
            CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
              PETSC_MATRIX%NUMBER_NON_ZEROS=PETSC_MATRIX%M*PETSC_MATRIX%GLOBAL_N
              PETSC_MATRIX%MAXIMUM_COLUMN_INDICES_PER_ROW=PETSC_MATRIX%GLOBAL_N
              PETSC_MATRIX%DATA_SIZE=PETSC_MATRIX%NUMBER_NON_ZEROS
              !Set up the Local to Global mappings
              ALLOCATE(PETSC_MATRIX%GLOBAL_ROW_NUMBERS(PETSC_MATRIX%M),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate global row numbers for PETSc distributed matrix.",ERR,ERROR,*999)
              DO i=1,PETSC_MATRIX%M
                PETSC_MATRIX%GLOBAL_ROW_NUMBERS(i)=ROW_DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(i)-1 !PETSc uses 0 based indexing
              ENDDO !i
              !Set up the matrix
              ALLOCATE(PETSC_MATRIX%DATA_DP(PETSC_MATRIX%DATA_SIZE),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc matrix data.",ERR,ERROR,*999)
#if ( PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 3 )
              CALL PETSC_MATCREATEDENSE(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_MATRIX%DATA_DP,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#else
              CALL PETSC_MATCREATEMPIDENSE(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_MATRIX%DATA_DP,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#endif              
            CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
              PETSC_MATRIX%NUMBER_NON_ZEROS=PETSC_MATRIX%M
              PETSC_MATRIX%MAXIMUM_COLUMN_INDICES_PER_ROW=1
              PETSC_MATRIX%DATA_SIZE=PETSC_MATRIX%NUMBER_NON_ZEROS
              !Set up the Local to Global mappings
              ALLOCATE(PETSC_MATRIX%GLOBAL_ROW_NUMBERS(PETSC_MATRIX%M),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate global row numbers for PETSc distributed matrix.",ERR,ERROR,*999)
              DO i=1,PETSC_MATRIX%M
                PETSC_MATRIX%GLOBAL_ROW_NUMBERS(i)=ROW_DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(i)-1 !PETSc uses 0 based indexing
              ENDDO !i
              !Set up the matrix
              ALLOCATE(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(PETSC_MATRIX%N),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate diagonal number of non zeros.",ERR,ERROR,*999)
              ALLOCATE(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS(PETSC_MATRIX%N),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate off diagonal number of non zeros.",ERR,ERROR,*999)
              PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS=1
              PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS=0
              !Create the PETsc AIJ matrix
#if ( PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 3 )
              CALL PETSC_MATCREATEAIJ(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_NULL_INTEGER,PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS, &
                & PETSC_NULL_INTEGER,PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#else              
              CALL PETSC_MATCREATEMPIAIJ(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_NULL_INTEGER,PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS, &
                & PETSC_NULL_INTEGER,PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#endif              
            CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
              IF(ALLOCATED(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS)) THEN
                IF(ALLOCATED(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS)) THEN
                  !Create the PETSc AIJ matrix
#if ( PETSC_VERSION_MAJOR >= 3 && PETSC_VERSION_MINOR >= 3 )
                  CALL PETSC_MATCREATEAIJ(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                    & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_NULL_INTEGER,PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS, &
                    & PETSC_NULL_INTEGER,PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#else
                  CALL PETSC_MATCREATEMPIAIJ(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_MATRIX%M,PETSC_MATRIX%N, &
                    & PETSC_MATRIX%GLOBAL_M,PETSC_MATRIX%GLOBAL_N,PETSC_NULL_INTEGER,PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS, &
                    & PETSC_NULL_INTEGER,PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS,PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
#endif                  
                  !Set matrix options
                  CALL PETSC_MATSETOPTION(PETSC_MATRIX%MATRIX,PETSC_MAT_NEW_NONZERO_LOCATION_ERR,PETSC_TRUE,ERR,ERROR,*999)
                  CALL PETSC_MATSETOPTION(PETSC_MATRIX%MATRIX,PETSC_MAT_NEW_NONZERO_ALLOCATION_ERR,PETSC_TRUE,ERR,ERROR,*999)
                  CALL PETSC_MATSETOPTION(PETSC_MATRIX%MATRIX,PETSC_MAT_UNUSED_NONZERO_LOCATION_ERR,PETSC_TRUE,ERR,ERROR,*999)
                  !Set up the Local to Global mappings
                  ALLOCATE(PETSC_MATRIX%GLOBAL_ROW_NUMBERS(PETSC_MATRIX%M),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate global row numbers for PETSc distributed matrix.",ERR,ERROR,*999)
                  PETSC_MATRIX%MAXIMUM_COLUMN_INDICES_PER_ROW=0
                  DO i=1,PETSC_MATRIX%M
                    PETSC_MATRIX%GLOBAL_ROW_NUMBERS(i)=ROW_DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(i)-1 !PETSc uses 0 based indexing
                    IF((PETSC_MATRIX%ROW_INDICES(i+1)-PETSC_MATRIX%ROW_INDICES(i))>PETSC_MATRIX%MAXIMUM_COLUMN_INDICES_PER_ROW) &
                      & PETSC_MATRIX%MAXIMUM_COLUMN_INDICES_PER_ROW=PETSC_MATRIX%ROW_INDICES(i+1)-PETSC_MATRIX%ROW_INDICES(i)
                  ENDDO !i
                ELSE
                  CALL FLAG_ERROR("Matrix off diagonal storage locations have not been set.",ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Matrix diagonal storage locations have not been set.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The PETSc matrix storage type of "//TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%STORAGE_TYPE,"*",ERR,ERROR))// &
                & " is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("PETSc matrix distributed matrix column domain mapping is not associated.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("PETSc matrix distributed matrix row domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH")
#endif
    RETURN
999 CALL DISTRIBUTED_MATRIX_PETSC_FINALISE(PETSC_MATRIX,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_PETSC_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Finalise a PETSc distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_PETSC_FINALISE(PETSC_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: PETSC_MATRIX !<A pointer to the PETSc distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_PETSC_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(PETSC_MATRIX)) THEN
      IF(ALLOCATED(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS)) DEALLOCATE(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS)
      IF(ALLOCATED(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS)) DEALLOCATE(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS)
      IF(ALLOCATED(PETSC_MATRIX%ROW_INDICES)) DEALLOCATE(PETSC_MATRIX%ROW_INDICES)
      IF(ALLOCATED(PETSC_MATRIX%COLUMN_INDICES)) DEALLOCATE(PETSC_MATRIX%COLUMN_INDICES)
      IF(ALLOCATED(PETSC_MATRIX%GLOBAL_ROW_NUMBERS)) DEALLOCATE(PETSC_MATRIX%GLOBAL_ROW_NUMBERS)
      !IF(ALLOCATED(PETSC_MATRIX%DATA_DP)) DEALLOCATE(PETSC_MATRIX%DATA_DP)
      CALL PETSC_MATFINALISE(PETSC_MATRIX%MATRIX,ERR,ERROR,*999)
      CALL PETSC_MATFINALISE(PETSC_MATRIX%OVERRIDE_MATRIX,ERR,ERROR,*999)
      DEALLOCATE(PETSC_MATRIX)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_FINALSE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_PETSC_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_PETSC_FINALISE

  !
  !================================================================================================================================
  !

  !>Intialises a PETSc distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_PETSC_INITIALISE(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_PETSC_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
        CALL FLAG_ERROR("PETSc is already associated for this distributed matrix",ERR,ERROR,*998)
      ELSE
        ROW_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
        COLUMN_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
        IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
          IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
            ALLOCATE(DISTRIBUTED_MATRIX%PETSC,STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc distributed matrix.",ERR,ERROR,*999)
            DISTRIBUTED_MATRIX%PETSC%DISTRIBUTED_MATRIX=>DISTRIBUTED_MATRIX
            DISTRIBUTED_MATRIX%LIBRARY_TYPE=DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE
            !Set the defaults          
            SELECT CASE(DISTRIBUTED_MATRIX%GHOSTING_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
              DISTRIBUTED_MATRIX%PETSC%M=ROW_DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL            
            CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
              DISTRIBUTED_MATRIX%PETSC%M=ROW_DOMAIN_MAPPING%NUMBER_OF_LOCAL
            CASE DEFAULT
              LOCAL_ERROR="The distributed matrix ghosting type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
            DISTRIBUTED_MATRIX%PETSC%N=COLUMN_DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
            DISTRIBUTED_MATRIX%PETSC%GLOBAL_M=ROW_DOMAIN_MAPPING%NUMBER_OF_GLOBAL
            DISTRIBUTED_MATRIX%PETSC%GLOBAL_N=COLUMN_DOMAIN_MAPPING%NUMBER_OF_GLOBAL
            DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE=DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE
            DISTRIBUTED_MATRIX%PETSC%DATA_SIZE=0
            DISTRIBUTED_MATRIX%PETSC%MAXIMUM_COLUMN_INDICES_PER_ROW=0
            DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX=.FALSE.
            CALL PETSC_MATINITIALISE(DISTRIBUTED_MATRIX%PETSC%MATRIX,ERR,ERROR,*999)
            CALL PETSC_MATINITIALISE(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix column domain mapping is not associated.",ERR,ERROR,*998)
          ENDIF
        ELSE
          CALL FLAG_ERROR("Distributed matrix row domain mapping is not associated.",ERR,ERROR,*998)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_INITIALSE")
#endif
    RETURN
999 IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) &
      & CALL DISTRIBUTED_MATRIX_PETSC_FINALISE(DISTRIBUTED_MATRIX%PETSC,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_MATRIX_PETSC_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_PETSC_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_PETSC_INITIALISE

  !
  !================================================================================================================================
  !

  !>Gets the storage locations (sparsity pattern) for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), POINTER :: ROW_INDICES(:) !<ROW_INDICES(i). On return, the i'th row index of the matrix storage locations
    INTEGER(INTG), POINTER :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). On return, the i'th column index of the matrix storage locations
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(DISTRIBUTED_MATRIX_CMISS_TYPE), POINTER :: CMISS_MATRIX
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: PETSC_MATRIX
    TYPE(VARYING_STRING) :: LOCAL_ERROR
     
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CMISS_MATRIX=>DISTRIBUTED_MATRIX%CMISS
          IF(ASSOCIATED(CMISS_MATRIX)) THEN
            CALL MATRIX_STORAGE_LOCATIONS_GET(CMISS_MATRIX%MATRIX,ROW_INDICES,COLUMN_INDICES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          PETSC_MATRIX=>DISTRIBUTED_MATRIX%PETSC
          IF(ASSOCIATED(PETSC_MATRIX)) THEN
            SELECT CASE(PETSC_MATRIX%STORAGE_TYPE)
            CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
              CALL FLAG_ERROR("Cannot get matrix locations for a block storage matrix.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
              CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
              ROW_INDICES=>DISTRIBUTED_MATRIX%PETSC%ROW_INDICES
              COLUMN_INDICES=>DISTRIBUTED_MATRIX%PETSC%COLUMN_INDICES
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The matrix storage type of "// &
                & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_GET

  !
  !================================================================================================================================
  !

  !>Sets the storage locations (sparsity pattern) in a distributed matrix to that specified by the row and column indices.
  SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index of the matrix storage locations
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index of the matrix storage locations
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i,j,k,global_row_start,global_row_finish
    TYPE(DISTRIBUTED_MATRIX_CMISS_TYPE), POINTER :: CMISS_MATRIX
    TYPE(DISTRIBUTED_MATRIX_PETSC_TYPE), POINTER :: PETSC_MATRIX
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_DOMAIN_MAPPING,COLUMN_DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: LOCAL_ERROR

    NULLIFY(CMISS_MATRIX)
    NULLIFY(PETSC_MATRIX)
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*999)
      ELSE
        ROW_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
        COLUMN_DOMAIN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
        IF(ASSOCIATED(ROW_DOMAIN_MAPPING)) THEN
          IF(ASSOCIATED(COLUMN_DOMAIN_MAPPING)) THEN
            SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              CMISS_MATRIX=>DISTRIBUTED_MATRIX%CMISS
              IF(ASSOCIATED(CMISS_MATRIX)) THEN
                CALL MATRIX_STORAGE_LOCATIONS_SET(CMISS_MATRIX%MATRIX,ROW_INDICES,COLUMN_INDICES,ERR,ERROR,*999)
              ELSE
                CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              PETSC_MATRIX=>DISTRIBUTED_MATRIX%PETSC
              IF(ASSOCIATED(PETSC_MATRIX)) THEN
                SELECT CASE(PETSC_MATRIX%STORAGE_TYPE)
                CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
                  !Do nothing
                CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
                  CALL FLAG_ERROR("Diagonal storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                  IF(SIZE(ROW_INDICES,1)==PETSC_MATRIX%M+1) THEN
                    IF(SIZE(COLUMN_INDICES,1)==PETSC_MATRIX%NUMBER_NON_ZEROS) THEN
                      IF(ROW_INDICES(1)==1) THEN
                        !Check the row indicies are correct
                        IF(ROW_INDICES(PETSC_MATRIX%M+1)==PETSC_MATRIX%NUMBER_NON_ZEROS+1) THEN
                          DO i=2,PETSC_MATRIX%M+1
                            IF(ROW_INDICES(i)<ROW_INDICES(i-1)) THEN
                              LOCAL_ERROR="Invalid row indices. Row "//TRIM(NUMBER_TO_VSTRING(i,"*",ERR,ERROR))// &
                                & " index number ("//TRIM(NUMBER_TO_VSTRING(ROW_INDICES(i),"*",ERR,ERROR))//" &
                                & ) is less than row "//TRIM(NUMBER_TO_VSTRING(i-1,"*",ERR,ERROR))//" index number ("// &
                                & TRIM(NUMBER_TO_VSTRING(ROW_INDICES(i-1),"*",ERR,ERROR))//")."
                              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                            ENDIF
                          ENDDO !i
                          !Allocate the PETSc sparsity storage arrays
                          ALLOCATE(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(PETSC_MATRIX%M),STAT=ERR)
                          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc matrix diagonal number of non zeros.",ERR,ERROR,*999)
                          PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS=0
                          ALLOCATE(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS(PETSC_MATRIX%M),STAT=ERR)
                          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc matrix off diagonal number of non zeros.", &
                            & ERR,ERROR,*999)
                          PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS=0
                          ALLOCATE(PETSC_MATRIX%ROW_INDICES(PETSC_MATRIX%M+1),STAT=ERR)
                          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc matrix row indices.",ERR,ERROR,*999)
                          PETSC_MATRIX%ROW_INDICES(1:PETSC_MATRIX%M+1)=ROW_INDICES(1:PETSC_MATRIX%M+1)
                          ALLOCATE(PETSC_MATRIX%COLUMN_INDICES(PETSC_MATRIX%NUMBER_NON_ZEROS),STAT=ERR)
                          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc matrix column indices.",ERR,ERROR,*999)
                          PETSC_MATRIX%COLUMN_INDICES(1:PETSC_MATRIX%NUMBER_NON_ZEROS)= &
                            & COLUMN_INDICES(1:PETSC_MATRIX%NUMBER_NON_ZEROS)
                          !Check the column indices are correct and calculate number of diagonal and off-diagonal columns
                          global_row_start=ROW_DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(1)
                          global_row_finish=ROW_DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(PETSC_MATRIX%M)
                          DO i=1,PETSC_MATRIX%M
                            DO j=ROW_INDICES(i),ROW_INDICES(i+1)-1
                              k=COLUMN_INDICES(j)
                              IF(k>0) THEN
                                IF(k>PETSC_MATRIX%GLOBAL_N) THEN
                                  LOCAL_ERROR="Invalid column indices. Column index "//TRIM(NUMBER_TO_VSTRING(j,"*",ERR,ERROR))// &
                                    & " ("//TRIM(NUMBER_TO_VSTRING(k,"*",ERR,ERROR))// &
                                    & ") is greater than the number of columns ("// &
                                    & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%GLOBAL_N,"*",ERR,ERROR))//")."
                                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                ENDIF
                                IF(k>=global_row_start.AND.k<=global_row_finish) THEN
                                  PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(i)=PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(i)+1
                                ELSE
                                  PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS(i)=PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS(i)+1
                                ENDIF
                              ELSE
                                LOCAL_ERROR="Invalid column indices. Column index "//TRIM(NUMBER_TO_VSTRING(j,"*",ERR,ERROR))// &
                                  & " ("//TRIM(NUMBER_TO_VSTRING(k,"*",ERR,ERROR))//") is less than zero."
                                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                              ENDIF
                            ENDDO !j
                            !Enforce a place for the diagonal entry.
                            IF(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(i)==0) PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS(i)=1
                          ENDDO !i
                          IF(DIAGNOSTICS3) THEN
                            CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"PETSc distributed matrix sparsity:",ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Storage type = ",PETSC_MATRIX%STORAGE_TYPE, &
                              & ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  M = ",PETSC_MATRIX%M,ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  N = ",PETSC_MATRIX%N,ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Global M = ",PETSC_MATRIX%GLOBAL_M,ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Global N = ",PETSC_MATRIX%GLOBAL_N,ERR,ERROR,*999)
                            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Number of non zeros = ",PETSC_MATRIX% &
                              & NUMBER_NON_ZEROS,ERR,ERROR,*999)
                            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,PETSC_MATRIX%M,8,8,PETSC_MATRIX%&
                              & DIAGONAL_NUMBER_NON_ZEROS,'("  Diagonal number non zeros     :",8(X,I10))','(33X,8(X,I10))', &
                              & ERR,ERROR,*999)
                            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,PETSC_MATRIX%M,8,8,PETSC_MATRIX%&
                              & OFFDIAGONAL_NUMBER_NON_ZEROS,'("  Off-diagonal number non zeros :",8(X,I10))','(33X,8(X,I10))', &
                              & ERR,ERROR,*999)
                            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,PETSC_MATRIX%M+1,8,8,PETSC_MATRIX%&
                              & ROW_INDICES,'("  Row indices                   :",8(X,I10))','(33X,8(X,I10))', &
                              & ERR,ERROR,*999)
                            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,PETSC_MATRIX%NUMBER_NON_ZEROS,8,8,PETSC_MATRIX%&
                              & COLUMN_INDICES,'("  Column indices                :",8(X,I10))','(33X,8(X,I10))', &
                              & ERR,ERROR,*999)
                          ENDIF
                        ELSE
                          LOCAL_ERROR="Invalid row indices. The last row index ("// &
                            & TRIM(NUMBER_TO_VSTRING(ROW_INDICES(PETSC_MATRIX%M+1),"*",ERR,ERROR))// &
                            & ") does not equal the number of non-zeros + 1 ("// &
                            & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%NUMBER_NON_ZEROS+1,"*",ERR,ERROR))//")."
                          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        LOCAL_ERROR="Invalid row indices. The first row index ("// &
                          & TRIM(NUMBER_TO_VSTRING(ROW_INDICES(1),"*",ERR,ERROR))//") does not equal 1."
                        CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      LOCAL_ERROR="The supplied number of column indices ("// &
                        & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                        & ") does not match the number of non-zeros in the matrix ("// &
                        & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%NUMBER_NON_ZEROS,"*",ERR,ERROR))//")."
                      CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                    ENDIF
                  ELSE
                    LOCAL_ERROR="The supplied number of row indices ("// &
                      & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                      & ") does not match the number of rows in the matrix + 1 ("// &
                      & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%M+1,"*",ERR,ERROR))//")."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
                  CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The specified matrix storage type of "// &
                    & TRIM(NUMBER_TO_VSTRING(PETSC_MATRIX%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ELSE
                CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed matrix library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("Distributed matrix column domain mapping is not associated.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("Distributed matrix row domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET")
#endif
    RETURN
999 IF(ASSOCIATED(PETSC_MATRIX)) THEN
      IF(ALLOCATED(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS)) DEALLOCATE(PETSC_MATRIX%DIAGONAL_NUMBER_NON_ZEROS)
      IF(ALLOCATED(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS)) DEALLOCATE(PETSC_MATRIX%OFFDIAGONAL_NUMBER_NON_ZEROS)
    ENDIF
    CALL ERRORS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_LOCATIONS_SET

  !
  !================================================================================================================================
  !

  !>Gets the storage type of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_TYPE_GET(DISTRIBUTED_MATRIX,STORAGE_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: STORAGE_TYPE !<On return, the storage (sparsity) type of the distributed matrix. \see MATRIX_VECTOR_StorageTypes,MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_STORAGE_TYPE_GET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_STORAGE_TYPE_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,STORAGE_TYPE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            STORAGE_TYPE=DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_TYPE_GET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_STORAGE_TYPE_GET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_TYPE_GET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_TYPE_GET

  !
  !================================================================================================================================
  !

  !>Sets/changes the storage type of a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_TYPE_SET(DISTRIBUTED_MATRIX,STORAGE_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: STORAGE_TYPE !<The storage (sparsity) type to set. \see MATRIX_VECTOR_StorageTypes,MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_STORAGE_TYPE_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        CALL FLAG_ERROR("The distributed matrix has been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_STORAGE_TYPE_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,STORAGE_TYPE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            SELECT CASE(STORAGE_TYPE)
            CASE(DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE)
              DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE=DISTRIBUTED_MATRIX_BLOCK_STORAGE_TYPE
            CASE(DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE)
              DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE=DISTRIBUTED_MATRIX_DIAGONAL_STORAGE_TYPE
            CASE(DISTRIBUTED_MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Column major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_MAJOR_STORAGE_TYPE)
              CALL FLAG_ERROR("Row major storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
              DISTRIBUTED_MATRIX%PETSC%STORAGE_TYPE=DISTRIBUTED_MATRIX_COMPRESSED_ROW_STORAGE_TYPE
            CASE(DISTRIBUTED_MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Compressed column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE(DISTRIBUTED_MATRIX_ROW_COLUMN_STORAGE_TYPE)
              CALL FLAG_ERROR("Row column storage is not implemented for PETSc matrices.",ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The specified matrix storage type of "//TRIM(NUMBER_TO_VSTRING(STORAGE_TYPE,"*",ERR,ERROR))// &
                & " is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not implemented.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_TYPE_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_STORAGE_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_STORAGE_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_STORAGE_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Finishes the update procedure for a distributed matrix. This routine will wait until all transfers have completed!
  SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_FINISH(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_UPDATE_FINISH",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          !Do nothing for now.
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
              CALL PETSC_MATASSEMBLYEND(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ELSE
              CALL PETSC_MATASSEMBLYEND(DISTRIBUTED_MATRIX%PETSC%MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_FINISH")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_UPDATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_FINISH

  !
  !================================================================================================================================
  !

  !>Tests to see if a distributed matrix update has finised.
  SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_ISFINISHED(DISTRIBUTED_MATRIX,ISFINISHED,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    LOGICAL, INTENT(OUT) :: ISFINISHED !<On return ISFINISHED will be .TRUE. if the distributed matrix update has finished or .FALSE. if it has not
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_UPDATE_ISFINISHED",ERR,ERROR,*999)
#endif

    ISFINISHED=.FALSE.
    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        !Do nothting for now.
        ISFINISHED=.TRUE.
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_ISFINISHED")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_UPDATE_ISFINISHED",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_ISFINISHED")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_ISFINISHED

  !
  !================================================================================================================================
  !

  !>Waits until a distributed matrix update has finised.
  SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        !Do nothing for now.
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_WAITFINISHED

  !
  !================================================================================================================================
  !

  !>Starts the update procedure for a distributed matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_START(DISTRIBUTED_MATRIX,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_UPDATE_START",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          !Do nothing for now.
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
              CALL PETSC_MATASSEMBLYBEGIN(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ELSE
              CALL PETSC_MATASSEMBLYBEGIN(DISTRIBUTED_MATRIX%PETSC%MATRIX,PETSC_MAT_FINAL_ASSEMBLY,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
     ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_START")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_UPDATE_START",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_UPDATE_START")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_UPDATE_START

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to add
    INTEGER(INTG), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to an integer PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to add a value to 
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to add a value to
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value to add at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to an integer PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG1

  !
  !================================================================================================================================
  !

  !>Adds a matrix of values to a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to add
    INTEGER(INTG), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to an integer PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_INTG2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_INTG2

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to add
    REAL(SP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a single precision PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to add a value to 
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to add a value to
    REAL(SP), INTENT(IN) :: VALUE !<The value to add at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a single precision PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP1

  !
  !================================================================================================================================
  !

  !>Adds a matrix of values to a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to add
    REAL(SP), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
   
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_SP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a single precision PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_SP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_SP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_SP2

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to add
    REAL(DP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,1)) THEN
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  DO i=1,SIZE(ROW_INDICES,1)
                    !Use global matrix row and column numbers
                    CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,1,DISTRIBUTED_MATRIX%PETSC% &
                      & GLOBAL_ROW_NUMBERS(ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i),PETSC_ADD_VALUES, &
                      & ERR,ERROR,*999) !PETSc uses 0 indicies
                  ENDDO !i
                ELSE
                  DO i=1,SIZE(ROW_INDICES,1)
                    !Use global matrix row and column numbers
                    CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,1,DISTRIBUTED_MATRIX%PETSC% &
                      & GLOBAL_ROW_NUMBERS(ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i),PETSC_ADD_VALUES, &
                      & ERR,ERROR,*999) !PETSc uses 0 indicies
                  ENDDO !i
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the size of the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the size of the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("The distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to add a value to 
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to add a value to
    REAL(DP), INTENT(IN) :: VALUE !<The value to add at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: PETSC_COL_INDEX(1)
    REAL(DP) :: PETSC_VALUE(1)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            !Use global matrix row and column numbers
            PETSC_COL_INDEX(1)=COLUMN_INDEX-1
            PETSC_VALUE(1)=VALUE
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
              CALL PETSC_MATSETVALUE(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS( &
                & ROW_INDEX),COLUMN_INDEX-1,VALUE,PETSC_ADD_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ELSE
              CALL PETSC_MATSETVALUE(DISTRIBUTED_MATRIX%PETSC%MATRIX,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS( &
                & ROW_INDEX),COLUMN_INDEX-1,VALUE,PETSC_ADD_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP1

  !
  !================================================================================================================================
  !

  !>Adds a matrix of values to a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to add
    REAL(DP), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: GLOBAL_ROW_INDICES(SIZE(ROW_INDICES)),i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_DP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,2)) THEN                
                DO i=1,SIZE(ROW_INDICES,1)
                  GLOBAL_ROW_INDICES(i)=DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS(ROW_INDICES(i))
                ENDDO !i
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,PETSC_ADD_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
                ELSE
                  CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,PETSC_ADD_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the number of columns in the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,2),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the number of rows in the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("The distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_DP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_DP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_DP2

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to add
    LOGICAL, INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a logical PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to add a value to 
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to add a value to
    LOGICAL, INTENT(IN) :: VALUE !<The value to add at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a logical PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L1

  !
  !================================================================================================================================
  !

  !>Adds a matrix of values to a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to add
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to add
    LOGICAL, INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_ADD_L2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_ADD(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Adding values to a logical PETSc distributed matrix is not implemented.",ERR,ERROR,*999)
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_ADD_L2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_ADD_L2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_ADD_L2

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to get
    INTEGER(INTG), INTENT(OUT) :: VALUES(:) !<VALUES(i). On return the i'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to get a value from
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to get a value from
    INTEGER(INTG), INTENT(OUT) :: VALUE !<On return the value of the matrix at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG1

  !
  !================================================================================================================================
  !

  !>Gets a matrix of values in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to get
    INTEGER(INTG), INTENT(OUT) :: VALUES(:,:) !<VALUES(i,j). On return the ij'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_INTG2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_INTG2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_INTG2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_INTG2

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<The row index to get a value from
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<The column index to get a value from
    REAL(SP), INTENT(OUT) :: VALUES(:) !<On return the value of the matrix at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to get a value from
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to get a value from
    REAL(SP), INTENT(OUT) :: VALUE !<On return the value of the matrix at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP1

  !
  !================================================================================================================================
  !

  !>Gets a matrix of values in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to get
    REAL(SP), INTENT(OUT) :: VALUES(:,:) !<VALUES(i,j). On return the ij'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_SP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_SP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_SP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_SP2

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to get
    REAL(DP), INTENT(OUT) :: VALUES(:) !<VALUES(i). On return the i'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,1)) THEN
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  DO i=1,SIZE(ROW_INDICES,1)
                    CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,1,DISTRIBUTED_MATRIX%PETSC% &
                      & GLOBAL_ROW_NUMBERS(ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i), &
                      & ERR,ERROR,*999) !PETSc uses 0 based indices
                  ENDDO !i
                ELSE
                  DO i=1,SIZE(ROW_INDICES,1)
                    CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,1,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS( &
                      & ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i),ERR,ERROR,*999) !PETSc uses 0 based indices
                  ENDDO !i
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the size of the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the size of the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF            
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to get a value from
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to get a value from
    REAL(DP), INTENT(OUT) :: VALUE !<On return the value of the matrix at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: COLUMN_INDICES(1)
    REAL(DP) :: VALUES(1)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            COLUMN_INDICES(1)=COLUMN_INDEX-1
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN              
              CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,1,DISTRIBUTED_MATRIX%PETSC% &
                & GLOBAL_ROW_NUMBERS(ROW_INDEX),1,COLUMN_INDICES,VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ELSE
              CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,1,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS(ROW_INDEX), &
                & 1,COLUMN_INDICES,VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ENDIF
            VALUE=VALUES(1)
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP1

  !
  !
  !================================================================================================================================
  !

  !>Gets a matrix of values in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to get
    REAL(DP), INTENT(OUT) :: VALUES(:,:) !<VALUES(i,j). On return the ij'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: GLOBAL_ROW_INDICES(SIZE(ROW_INDICES,1)),i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_DP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer precision PETSc distributed matrix.",ERR,ERROR,*999)          
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,2)) THEN
                DO i=1,SIZE(ROW_INDICES,1)
                  GLOBAL_ROW_INDICES(i)=DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS(ROW_INDICES(i))
                ENDDO !i
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,ERR,ERROR,*999) !PETSc uses 0 based row indices
                ELSE
                  CALL PETSC_MATGETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,ERR,ERROR,*999) !PETSc uses 0 based row indices
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the number of columns in the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,2),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the number of rows in the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_DP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_DP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_DP2

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to get
    LOGICAL, INTENT(OUT) :: VALUES(:) !<VALUES(i). On return the i'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to get a value from
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to get a value from
    LOGICAL, INTENT(OUT) :: VALUE !<On return the value of the matrix at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
   
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L1

  !
  !================================================================================================================================
  !

  !>Gets a matrix of values in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to get
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to get
    LOGICAL, INTENT(OUT) :: VALUES(:,:) !<VALUES(i,j). On return the ij'th value to get
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_GET_L2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_GET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed matrix.",ERR,ERROR,*999)                    
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_GET_L2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_GET_L2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_GET_L2

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to set
    INTEGER(INTG), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to set a value to
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to set a value to
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value of the matrix to be set at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG1

  !
  !================================================================================================================================
  !

  !>Sets a matrix of values in a distributed integer matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to set
    INTEGER(INTG), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_INTG2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_INTG2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_INTG2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_INTG2

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to set
    REAL(SP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to set a value to
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to set a value to
    REAL(SP), INTENT(IN) :: VALUE !<The value of the matrix to be set at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP1

  !
  !================================================================================================================================
  !

  !>Sets a matrix of values in a distributed single precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to set
    REAL(SP), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_SP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_SP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_SP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_SP2

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to set
    REAL(DP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,1)) THEN
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  DO i=1,SIZE(ROW_INDICES,1)
                    CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,1,DISTRIBUTED_MATRIX%PETSC% &
                      & GLOBAL_ROW_NUMBERS(ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i),PETSC_INSERT_VALUES, &
                      & ERR,ERROR,*999) !0 based indices
                  ENDDO !i
                ELSE
                  DO i=1,SIZE(ROW_INDICES,1)
                    CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,1,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS( &
                      & ROW_INDICES(i:i)),1,COLUMN_INDICES(i:i)-1,VALUES(i:i),PETSC_INSERT_VALUES,ERR,ERROR,*999) !0 based indices
                  ENDDO !i
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the size of the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the size of the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF            
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to set a value to
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to set a value to
    REAL(DP), INTENT(IN) :: VALUE !<The value of the matrix to be set at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
              CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,1,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS( &
                & ROW_INDEX),1,(/COLUMN_INDEX-1/),(/VALUE/),PETSC_INSERT_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ELSE
              CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,1,DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS(ROW_INDEX), &
                & 1,(/COLUMN_INDEX-1/),(/VALUE/),PETSC_INSERT_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP1

  !
  !================================================================================================================================
  !

  !>Sets a matrix of values in a distributed double precision matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to set
    REAL(DP), INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: GLOBAL_ROW_INDICES(SIZE(ROW_INDICES,1)),i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_DP2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%PETSC)) THEN
            IF(SIZE(ROW_INDICES,1)==SIZE(VALUES,1)) THEN
              IF(SIZE(COLUMN_INDICES,1)==SIZE(VALUES,2)) THEN
                DO i=1,SIZE(ROW_INDICES,1)
                  GLOBAL_ROW_INDICES(i)=DISTRIBUTED_MATRIX%PETSC%GLOBAL_ROW_NUMBERS(ROW_INDICES(i))
                ENDDO !i
                IF(DISTRIBUTED_MATRIX%PETSC%USE_OVERRIDE_MATRIX) THEN
                  CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%OVERRIDE_MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,PETSC_INSERT_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
                ELSE
                  CALL PETSC_MATSETVALUES(DISTRIBUTED_MATRIX%PETSC%MATRIX,SIZE(ROW_INDICES,1),GLOBAL_ROW_INDICES, &
                    & SIZE(COLUMN_INDICES,1),COLUMN_INDICES-1,VALUES,PETSC_INSERT_VALUES,ERR,ERROR,*999) !PETSc uses 0 based indices
                ENDIF
              ELSE
                LOCAL_ERROR="The size of the column indices array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(COLUMN_INDICES,1),"*",ERR,ERROR))// &
                  & ") does not conform to the number of columns in the values array ("// &
                  & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,2),"*",ERR,ERROR))//")."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The size of the row indices array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(ROW_INDICES,1),"*",ERR,ERROR))// &
                & ") does not conform to the number of rows in the values array ("// &
                & TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("The distributed matrix PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_DP2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_DP2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_DP2

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The i'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(i). The i'th column index to set
    LOGICAL, INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set values for a logical PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L1(DISTRIBUTED_MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDEX !<The row index to set a value to
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDEX !<The column index to set a value to
    LOGICAL, INTENT(IN) :: VALUE !<The value of the matrix to be set at the specified row and column
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDEX,COLUMN_INDEX,VALUE,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L1

  !
  !================================================================================================================================
  !

  !>Sets a matrix of values in a distributed logical matrix.
  SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L2(DISTRIBUTED_MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    INTEGER(INTG), INTENT(IN) :: ROW_INDICES(:) !<ROW_INDICES(i). The ij'th row index to set
    INTEGER(INTG), INTENT(IN) :: COLUMN_INDICES(:) !<COLUMN_INDICES(j). The ij'th column index to set
    LOGICAL, INTENT(IN) :: VALUES(:,:) !<VALUES(i,j). The ij'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_VALUES_SET_L2",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
      IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_MATRIX%CMISS)) THEN
            CALL MATRIX_VALUES_SET(DISTRIBUTED_MATRIX%CMISS%MATRIX,ROW_INDICES,COLUMN_INDICES,VALUES,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed matrix CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed matrix.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed matrix library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed matrix has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed matrix is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L2")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_VALUES_SET_L2",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_VALUES_SET_L2")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_VALUES_SET_L2

  !
  !================================================================================================================================
  !

  !>Calculates the matrix vector product of a distrubted matrix times a distributed vector and adds it to the distributed
  !>product vector. NOTE: This will only work for specific CMISS distributed matrices i.e., ones in which the columns of the
  !>matrix are distributed in the same way as the rows of the multiplied vector are distributed, and the rows of the matrix
  !>are distributed in the same way as the rows of the product vector.
  SUBROUTINE DISTRIBUTED_MATRIX_BY_VECTOR_ADD(ROW_SELECTION_TYPE,ALPHA,DISTRIBUTED_MATRIX,DISTRIBUTED_VECTOR,DISTRIBUTED_PRODUCT, &
    & ERR,ERROR,*)

    !Argument variables
    INTEGER(INTG), INTENT(IN) :: ROW_SELECTION_TYPE !<The row selection for the matrix-vector product \see DISTRIBUTED_MATRIX_VECTOR_GhostingTypes,DISTRIBUTED_MATRIX_VECTOR
    REAL(DP), INTENT(IN) :: ALPHA !<The multiplicative factor for the distributed matrix
    TYPE(DISTRIBUTED_MATRIX_TYPE), POINTER :: DISTRIBUTED_MATRIX !<A pointer to the distributed matrix
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_PRODUCT !<On exit, the value of the matrix vector product added to the distributed product vector.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: column_idx,local_column,global_column,NUMBER_OF_COLUMNS,NUMBER_OF_ROWS,row,row_idx
    REAL(DP) :: SUM
    TYPE(DISTRIBUTED_MATRIX_CMISS_TYPE), POINTER :: CMISS_MATRIX
    TYPE(DISTRIBUTED_VECTOR_CMISS_TYPE), POINTER :: CMISS_VECTOR,CMISS_PRODUCT
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: ROW_MAPPING,COLUMN_MAPPING
    TYPE(MATRIX_TYPE), POINTER :: MATRIX
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_MATRIX_BY_VECTOR_ADD",ERR,ERROR,*999)
#endif

    IF(ABS(ALPHA)>ZERO_TOLERANCE) THEN
      IF(ASSOCIATED(DISTRIBUTED_MATRIX)) THEN
        IF(DISTRIBUTED_MATRIX%MATRIX_FINISHED) THEN
          IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
            IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
              IF(ASSOCIATED(DISTRIBUTED_PRODUCT)) THEN
                IF(DISTRIBUTED_PRODUCT%VECTOR_FINISHED) THEN
                  IF(DISTRIBUTED_MATRIX%LIBRARY_TYPE==DISTRIBUTED_VECTOR%LIBRARY_TYPE) THEN
                    IF(DISTRIBUTED_MATRIX%LIBRARY_TYPE==DISTRIBUTED_PRODUCT%LIBRARY_TYPE) THEN
                      COLUMN_MAPPING=>DISTRIBUTED_MATRIX%COLUMN_DOMAIN_MAPPING
                      IF(ASSOCIATED(COLUMN_MAPPING)) THEN
                        ROW_MAPPING=>DISTRIBUTED_MATRIX%ROW_DOMAIN_MAPPING
                        IF(ASSOCIATED(ROW_MAPPING)) THEN
                          IF(ASSOCIATED(COLUMN_MAPPING,DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
                            IF(ASSOCIATED(ROW_MAPPING,DISTRIBUTED_PRODUCT%DOMAIN_MAPPING)) THEN
                              SELECT CASE(DISTRIBUTED_MATRIX%LIBRARY_TYPE)
                              CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
                                CMISS_MATRIX=>DISTRIBUTED_MATRIX%CMISS
                                IF(ASSOCIATED(CMISS_MATRIX)) THEN
                                  MATRIX=>CMISS_MATRIX%MATRIX
                                  IF(ASSOCIATED(MATRIX)) THEN
                                    CMISS_VECTOR=>DISTRIBUTED_VECTOR%CMISS
                                    IF(ASSOCIATED(CMISS_VECTOR)) THEN
                                      CMISS_PRODUCT=>DISTRIBUTED_PRODUCT%CMISS
                                      IF(ASSOCIATED(CMISS_PRODUCT)) THEN
                                        SELECT CASE(ROW_SELECTION_TYPE)
                                        CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
                                          NUMBER_OF_ROWS=ROW_MAPPING%TOTAL_NUMBER_OF_LOCAL
                                        CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
                                          NUMBER_OF_ROWS=ROW_MAPPING%NUMBER_OF_LOCAL
                                        CASE DEFAULT
                                          LOCAL_ERROR="The row selection type of "// &
                                            & TRIM(NUMBER_TO_VSTRING(ROW_SELECTION_TYPE,"*",ERR,ERROR))//" is invalid."
                                          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                        END SELECT
                                        NUMBER_OF_COLUMNS=COLUMN_MAPPING%NUMBER_OF_GLOBAL
                                        IF(MATRIX%DATA_TYPE==DISTRIBUTED_VECTOR%DATA_TYPE) THEN
                                          IF(MATRIX%DATA_TYPE==DISTRIBUTED_PRODUCT%DATA_TYPE) THEN
                                            SELECT CASE(MATRIX%DATA_TYPE)
                                            CASE(DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE)
                                              CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                                            CASE(DISTRIBUTED_MATRIX_VECTOR_SP_TYPE)
                                              CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                                            CASE(DISTRIBUTED_MATRIX_VECTOR_DP_TYPE)
                                              SELECT CASE(MATRIX%STORAGE_TYPE)
                                              CASE(MATRIX_BLOCK_STORAGE_TYPE)
                                                DO row=1,NUMBER_OF_ROWS
                                                  SUM=0.0_DP
                                                  DO local_column=1,COLUMN_MAPPING%TOTAL_NUMBER_OF_LOCAL
                                                    global_column=COLUMN_MAPPING%LOCAL_TO_GLOBAL_MAP(local_column)
                                                    SUM=SUM+MATRIX%DATA_DP(row+(global_column-1)*MATRIX%M)* &
                                                      & CMISS_VECTOR%DATA_DP(local_column)
                                                  ENDDO !local_column
                                                  CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                ENDDO !row
                                              CASE(MATRIX_DIAGONAL_STORAGE_TYPE)
                                                DO row=1,NUMBER_OF_ROWS
                                                  SUM=MATRIX%DATA_DP(row)*CMISS_VECTOR%DATA_DP(row)
                                                  CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                ENDDO !row
                                              CASE(MATRIX_COLUMN_MAJOR_STORAGE_TYPE)
                                                DO row=1,NUMBER_OF_ROWS
                                                  SUM=0.0_DP
                                                  DO local_column=1,COLUMN_MAPPING%TOTAL_NUMBER_OF_LOCAL
                                                    global_column=COLUMN_MAPPING%LOCAL_TO_GLOBAL_MAP(local_column)
                                                    SUM=SUM+MATRIX%DATA_DP(row+(global_column-1)*MATRIX%MAX_M)* &
                                                      & CMISS_VECTOR%DATA_DP(local_column)
                                                  ENDDO !local_column
                                                  CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                ENDDO !row
                                              CASE(MATRIX_ROW_MAJOR_STORAGE_TYPE)
                                                DO row=1,NUMBER_OF_ROWS
                                                  SUM=0.0_DP
                                                  DO local_column=1,COLUMN_MAPPING%TOTAL_NUMBER_OF_LOCAL
                                                    global_column=COLUMN_MAPPING%LOCAL_TO_GLOBAL_MAP(local_column)
                                                    SUM=SUM+MATRIX%DATA_DP((row-1)*MATRIX%MAX_N+global_column)* &
                                                      & CMISS_VECTOR%DATA_DP(local_column)
                                                  ENDDO !local_column
                                                  CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                ENDDO !row
                                              CASE(MATRIX_COMPRESSED_ROW_STORAGE_TYPE)
                                                DO row=1,NUMBER_OF_ROWS
                                                  SUM=0.0_DP
                                                  DO column_idx=MATRIX%ROW_INDICES(row),MATRIX%ROW_INDICES(row+1)-1
                                                    global_column=MATRIX%COLUMN_INDICES(column_idx)
                                                    !This ranks global to local mappings are stored in the first position
                                                    local_column=COLUMN_MAPPING%GLOBAL_TO_LOCAL_MAP(global_column)%LOCAL_NUMBER(1)
                                                    SUM=SUM+MATRIX%DATA_DP(column_idx)* &
                                                      & CMISS_VECTOR%DATA_DP(local_column)
                                                  ENDDO !local_column
                                                  CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                ENDDO !row
                                              CASE(MATRIX_COMPRESSED_COLUMN_STORAGE_TYPE)
                                                DO column_idx=1,NUMBER_OF_COLUMNS
                                                  DO row_idx=MATRIX%COLUMN_INDICES(column_idx),MATRIX%COLUMN_INDICES(column_idx+1)-1
                                                    row=MATRIX%ROW_INDICES(row_idx)
                                                    local_column=COLUMN_MAPPING%GLOBAL_TO_LOCAL_MAP(column_idx)%LOCAL_NUMBER(1)
                                                    SUM=MATRIX%DATA_DP(row_idx)* &
                                                      & CMISS_VECTOR%DATA_DP(local_column)
                                                    CMISS_PRODUCT%DATA_DP(row)=CMISS_PRODUCT%DATA_DP(row)+(ALPHA*SUM)
                                                  ENDDO !local_row
                                                ENDDO !column_idx
                                              CASE(MATRIX_ROW_COLUMN_STORAGE_TYPE)
                                                CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                                              CASE DEFAULT
                                                LOCAL_ERROR="The matrix storage type of "// &
                                                  & TRIM(NUMBER_TO_VSTRING(MATRIX%STORAGE_TYPE,"*",ERR,ERROR))//" is invalid."

                                                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                              END SELECT
                                            CASE(DISTRIBUTED_MATRIX_VECTOR_L_TYPE)
                                              CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                                            CASE DEFAULT
                                              LOCAL_ERROR="The distributed matrix vector data type of "// &
                                                & TRIM(NUMBER_TO_VSTRING(MATRIX%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                                              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                            END SELECT
                                          ELSE
                                            LOCAL_ERROR="The distributed product vector data type of "// &
                                              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_PRODUCT%DATA_TYPE,"*",ERR,ERROR))// &
                                              & " does not match the distributed matrix data type of "// &
                                              & TRIM(NUMBER_TO_VSTRING(MATRIX%DATA_TYPE,"*",ERR,ERROR))//"."
                                            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                          ENDIF
                                        ELSE
                                          LOCAL_ERROR="The distributed vector data type of "// &
                                            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                                            & " does not match the distributed matrix data type of "// &
                                            & TRIM(NUMBER_TO_VSTRING(MATRIX%DATA_TYPE,"*",ERR,ERROR))//"."
                                          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                                        ENDIF
                                      ELSE
                                        CALL FLAG_ERROR("Distributed product CMISS vector is not associated.",ERR,ERROR,*999)
                                      ENDIF
                                    ELSE
                                      CALL FLAG_ERROR("Distributed vector CMISS vector is not associated.",ERR,ERROR,*999)
                                    ENDIF
                                  ELSE
                                    CALL FLAG_ERROR("CMISS matrix matrix is not associated.",ERR,ERROR,*999)
                                  ENDIF
                                ELSE
                                  CALL FLAG_ERROR("Distrubuted matrix CMISS is not associated.",ERR,ERROR,*999)
                                ENDIF
                              CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
                                CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                              CASE DEFAULT
                                LOCAL_ERROR="The distributed matrix library type of "// &
                                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid"
                                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                              END SELECT
                            ELSE
                              CALL FLAG_ERROR("The distributed matrix and the distributed product vector have different "// &
                                & "domain mappings.",ERR,ERROR,*999)
                            ENDIF
                          ELSE
                            CALL FLAG_ERROR("The distributed matrix and the distributed vector have different domain mappings.", &
                              & ERR,ERROR,*999)
                          ENDIF
                        ELSE
                          CALL FLAG_ERROR("The distributed matrix row domain mapping is not associated.",ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("The distributed matrix column domain mapping is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      LOCAL_ERROR="The distributed product vector library type of "// &
                        & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_PRODUCT%LIBRARY_TYPE,"*",ERR,ERROR))// &
                        & " does not match the distributed matrix library type of "// &
                        &  TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//"."
                      CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                    ENDIF
                  ELSE
                    LOCAL_ERROR="The distributed vector library type of "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))// &
                      & " does not match the distributed matrix library type of "// &
                      &  TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_MATRIX%LIBRARY_TYPE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ELSE
                  CALL FLAG_ERROR("The distributed product vector has not been finished.",ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("The distributed product vector is not associated.",ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector has not been finished.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distrubuted vector is not associated.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("Distributed matrix has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Distributed matrix is not associated",ERR,ERROR,*999)
      ENDIF
    ENDIF
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_BY_VECTOR_ADD")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_MATRIX_BY_VECTOR_ADD",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_MATRIX_BY_VECTOR_ADD")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_MATRIX_BY_VECTOR_ADD
  
  !
  !================================================================================================================================
  !
  
  !>Sets all values in an integer distributed vector to the specified value.
  SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG(DISTRIBUTED_VECTOR,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              DISTRIBUTED_VECTOR%CMISS%DATA_INTG=VALUE
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The data type of "//TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the integer data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_INTG

  !
  !================================================================================================================================
  !

  !>Sets all values in a single precision distributed vector to the specified value.
  SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP(DISTRIBUTED_VECTOR,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(SP), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              DISTRIBUTED_VECTOR%CMISS%DATA_SP=VALUE
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The data type of "//TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the single precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_SP

  !
  !================================================================================================================================
  !

  !>Sets all values in a double precision distributed vector to the specified value.
  SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP(DISTRIBUTED_VECTOR,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(DP), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              DISTRIBUTED_VECTOR%CMISS%DATA_DP=VALUE
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                CALL PETSC_VECSET(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,VALUE,ERR,ERROR,*999)
              ELSE
                CALL PETSC_VECSET(DISTRIBUTED_VECTOR%PETSC%VECTOR,VALUE,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The data type of "//TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the double precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_DP

  !
  !================================================================================================================================
  !

  !>Sets all values in a logical distributed_vector to the specified value.
  SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_L(DISTRIBUTED_VECTOR,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    LOGICAL, INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              DISTRIBUTED_VECTOR%CMISS%DATA_L=VALUE
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The data type of "//TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the logical data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_ALL_VALUES_SET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_ALL_VALUES_SET_L

  !
  !================================================================================================================================
  !

  !>Copies alpha times an integer distributed vector to another distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_COPY_INTG(FROM_VECTOR,TO_VECTOR,ALPHA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: FROM_VECTOR !<A pointer to the distributed vector to copy from
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: TO_VECTOR !<A pointer to the distributed vector to copy to
    INTEGER(INTG), INTENT(IN) :: ALPHA !<The multiplicative factor for the copy.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_COPY_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(FROM_VECTOR)) THEN
      IF(FROM_VECTOR%VECTOR_FINISHED) THEN
        IF(ASSOCIATED(TO_VECTOR)) THEN
          IF(TO_VECTOR%VECTOR_FINISHED) THEN
            IF(FROM_VECTOR%DATA_TYPE==TO_VECTOR%DATA_TYPE) THEN
              IF(FROM_VECTOR%DATA_TYPE==DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE) THEN
                IF(FROM_VECTOR%LIBRARY_TYPE==TO_VECTOR%LIBRARY_TYPE) THEN
                  !Vectors are of the same library type
                  SELECT CASE(FROM_VECTOR%LIBRARY_TYPE)
                  CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
                    IF(ASSOCIATED(FROM_VECTOR%CMISS)) THEN
                      IF(ASSOCIATED(TO_VECTOR%CMISS)) THEN
                        IF(ASSOCIATED(FROM_VECTOR%DOMAIN_MAPPING,TO_VECTOR%DOMAIN_MAPPING)) THEN
                          TO_VECTOR%CMISS%DATA_INTG(1:TO_VECTOR%CMISS%N)=ALPHA*FROM_VECTOR%CMISS%DATA_INTG(1:FROM_VECTOR%CMISS%N)
                        ELSE
                          CALL FLAG_ERROR("The from vector does not have the same domain mapping as the to vector.",ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("To vector CMISS is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      CALL FLAG_ERROR("From vector CMISS is not associated.",ERR,ERROR,*999)
                    ENDIF
                  CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
                    CALL FLAG_ERROR("Cannot copy a vector fro an integer PETSc distributed vector.",ERR,ERROR,*999)
                  CASE DEFAULT
                    LOCAL_ERROR="The from vector library type of "// &
                      & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  END SELECT
                ELSE
                  !Vectors are of from different library types
                  CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                ENDIF
              ELSE
                LOCAL_ERROR="The from vector data type of "//TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                  & " does not match the integer data type of the supplied alpha value."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The from vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                & " does not match the to vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(TO_VECTOR%DATA_TYPE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("To vector has not been finished.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("To vector is not associated.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("From vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("From vector is not associated.",ERR,ERROR,*999)
    ENDIF
     
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_COPY_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_INTG")
#endif
    RETURN 1
    
  END SUBROUTINE DISTRIBUTED_VECTOR_COPY_INTG

  !
  !================================================================================================================================
  !

  !>Copies alpha times a double precision distributed vector to another distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_COPY_DP(FROM_VECTOR,TO_VECTOR,ALPHA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: FROM_VECTOR !<A pointer to the distributed vector to copy from
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: TO_VECTOR !<A pointer to the distributed vector to copy to
    REAL(DP), INTENT(IN) :: ALPHA !<The multiplicative factor for the copy.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_COPY_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(FROM_VECTOR)) THEN
      IF(FROM_VECTOR%VECTOR_FINISHED) THEN
        IF(ASSOCIATED(TO_VECTOR)) THEN
          IF(TO_VECTOR%VECTOR_FINISHED) THEN
            IF(FROM_VECTOR%DATA_TYPE==TO_VECTOR%DATA_TYPE) THEN
              IF(FROM_VECTOR%DATA_TYPE==DISTRIBUTED_MATRIX_VECTOR_DP_TYPE) THEN
                IF(FROM_VECTOR%LIBRARY_TYPE==TO_VECTOR%LIBRARY_TYPE) THEN
                  !Vectors are of the same library type
                  SELECT CASE(FROM_VECTOR%LIBRARY_TYPE)
                  CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
                    IF(ASSOCIATED(FROM_VECTOR%CMISS)) THEN
                      IF(ASSOCIATED(TO_VECTOR%CMISS)) THEN
                        IF(ASSOCIATED(FROM_VECTOR%DOMAIN_MAPPING,TO_VECTOR%DOMAIN_MAPPING)) THEN
                          TO_VECTOR%CMISS%DATA_DP(1:TO_VECTOR%CMISS%N)=ALPHA*FROM_VECTOR%CMISS%DATA_DP(1:FROM_VECTOR%CMISS%N)
                        ELSE
                          CALL FLAG_ERROR("The from vector does not have the same domain mapping as the to vector.",ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("To vector CMISS is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      CALL FLAG_ERROR("From vector CMISS is not associated.",ERR,ERROR,*999)
                    ENDIF
                  CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
                    IF(ASSOCIATED(FROM_VECTOR%PETSC)) THEN
                      IF(ASSOCIATED(TO_VECTOR%PETSC)) THEN
                        IF(FROM_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                          IF(TO_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                            CALL PETSC_VECCOPY(FROM_VECTOR%PETSC%OVERRIDE_VECTOR,TO_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)
                            CALL PETSC_VECSCALE(TO_VECTOR%PETSC%OVERRIDE_VECTOR,ALPHA,ERR,ERROR,*999)
                          ELSE
                            CALL PETSC_VECCOPY(FROM_VECTOR%PETSC%OVERRIDE_VECTOR,TO_VECTOR%PETSC%VECTOR,ERR,ERROR,*999)
                            CALL PETSC_VECSCALE(TO_VECTOR%PETSC%VECTOR,ALPHA,ERR,ERROR,*999)
                          ENDIF
                        ELSE
                          IF(TO_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                            CALL PETSC_VECCOPY(FROM_VECTOR%PETSC%VECTOR,TO_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)
                            CALL PETSC_VECSCALE(TO_VECTOR%PETSC%OVERRIDE_VECTOR,ALPHA,ERR,ERROR,*999)
                          ELSE
                            CALL PETSC_VECCOPY(FROM_VECTOR%PETSC%VECTOR,TO_VECTOR%PETSC%VECTOR,ERR,ERROR,*999)
                            CALL PETSC_VECSCALE(TO_VECTOR%PETSC%VECTOR,ALPHA,ERR,ERROR,*999)
                          ENDIF
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("To vector PETSc is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      CALL FLAG_ERROR("From vector PETSc is not associated.",ERR,ERROR,*999)
                    ENDIF
                  CASE DEFAULT
                    LOCAL_ERROR="The from vector library type of "// &
                      & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  END SELECT
                ELSE
                  !Vectors are of from different library types
                  CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                ENDIF
              ELSE
                LOCAL_ERROR="The from vector data type of "//TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                  & " does not match the double precision data type of the supplied alpha value."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The from vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                & " does not match the to vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(TO_VECTOR%DATA_TYPE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("To vector has not been finished.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("To vector is not associated.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("From vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("From vector is not associated.",ERR,ERROR,*999)
    ENDIF
     
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_COPY_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_DP")
#endif
    RETURN 1
    
  END SUBROUTINE DISTRIBUTED_VECTOR_COPY_DP

  !
  !================================================================================================================================
  !

  !>Copies alpha times a single precision distributed vector to another distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_COPY_SP(FROM_VECTOR,TO_VECTOR,ALPHA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: FROM_VECTOR !<A pointer to the distributed vector to copy from
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: TO_VECTOR !<A pointer to the distributed vector to copy to
    REAL(SP), INTENT(IN) :: ALPHA !<The multiplicative factor for the copy.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_COPY_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(FROM_VECTOR)) THEN
      IF(FROM_VECTOR%VECTOR_FINISHED) THEN
        IF(ASSOCIATED(TO_VECTOR)) THEN
          IF(TO_VECTOR%VECTOR_FINISHED) THEN
            IF(FROM_VECTOR%DATA_TYPE==TO_VECTOR%DATA_TYPE) THEN
              IF(FROM_VECTOR%DATA_TYPE==DISTRIBUTED_MATRIX_VECTOR_SP_TYPE) THEN
                IF(FROM_VECTOR%LIBRARY_TYPE==TO_VECTOR%LIBRARY_TYPE) THEN
                  !Vectors are of the same library type
                  SELECT CASE(FROM_VECTOR%LIBRARY_TYPE)
                  CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
                    IF(ASSOCIATED(FROM_VECTOR%CMISS)) THEN
                      IF(ASSOCIATED(TO_VECTOR%CMISS)) THEN
                        IF(ASSOCIATED(FROM_VECTOR%DOMAIN_MAPPING,TO_VECTOR%DOMAIN_MAPPING)) THEN
                          TO_VECTOR%CMISS%DATA_SP(1:TO_VECTOR%CMISS%N)=ALPHA*FROM_VECTOR%CMISS%DATA_SP(1:FROM_VECTOR%CMISS%N)
                        ELSE
                          CALL FLAG_ERROR("The from vector does not have the same domain mapping as the to vector.",ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("To vector CMISS is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      CALL FLAG_ERROR("From vector CMISS is not associated.",ERR,ERROR,*999)
                    ENDIF
                  CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
                    CALL FLAG_ERROR("Cannot copy a vector for a single precision PETSc distributed vector.",ERR,ERROR,*999)
                 CASE DEFAULT
                    LOCAL_ERROR="The from vector library type of "// &
                      & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  END SELECT
                ELSE
                  !Vectors are of from different library types
                  CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                ENDIF
              ELSE
                LOCAL_ERROR="The from vector data type of "//TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                  & " does not match the single precision data type of the supplied alpha value."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The from vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                & " does not match the to vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(TO_VECTOR%DATA_TYPE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("To vector has not been finished.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("To vector is not associated.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("From vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("From vector is not associated.",ERR,ERROR,*999)
    ENDIF
     
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_COPY_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_SP")
#endif
    RETURN 1
    
  END SUBROUTINE DISTRIBUTED_VECTOR_COPY_SP

  !
  !================================================================================================================================
  !

  !>Copies alpha times a logical distributed vector to another distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_COPY_L(FROM_VECTOR,TO_VECTOR,ALPHA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: FROM_VECTOR !<A pointer to the distributed vector to copy from
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: TO_VECTOR !<A pointer to the distributed vector to copy to
    LOGICAL, INTENT(IN) :: ALPHA !<The multiplicative factor for the copy.
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_COPY_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(FROM_VECTOR)) THEN
      IF(FROM_VECTOR%VECTOR_FINISHED) THEN
        IF(ASSOCIATED(TO_VECTOR)) THEN
          IF(TO_VECTOR%VECTOR_FINISHED) THEN
            IF(FROM_VECTOR%DATA_TYPE==TO_VECTOR%DATA_TYPE) THEN
              IF(FROM_VECTOR%DATA_TYPE==DISTRIBUTED_MATRIX_VECTOR_L_TYPE) THEN
                IF(FROM_VECTOR%LIBRARY_TYPE==TO_VECTOR%LIBRARY_TYPE) THEN
                  !Vectors are of the same library type
                  SELECT CASE(FROM_VECTOR%LIBRARY_TYPE)
                  CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
                    IF(ASSOCIATED(FROM_VECTOR%CMISS)) THEN
                      IF(ASSOCIATED(TO_VECTOR%CMISS)) THEN
                        IF(ASSOCIATED(FROM_VECTOR%DOMAIN_MAPPING,TO_VECTOR%DOMAIN_MAPPING)) THEN
                          TO_VECTOR%CMISS%DATA_L(1:TO_VECTOR%CMISS%N)=ALPHA.AND.FROM_VECTOR%CMISS%DATA_L(1:FROM_VECTOR%CMISS%N)
                        ELSE
                          CALL FLAG_ERROR("The from vector does not have the same domain mapping as the to vector.",ERR,ERROR,*999)
                        ENDIF
                      ELSE
                        CALL FLAG_ERROR("To vector CMISS is not associated.",ERR,ERROR,*999)
                      ENDIF
                    ELSE
                      CALL FLAG_ERROR("From vector CMISS is not associated.",ERR,ERROR,*999)
                    ENDIF
                  CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
                    CALL FLAG_ERROR("Cannot copy a vector for an integer PETSc distributed vector.",ERR,ERROR,*999)
                  CASE DEFAULT
                    LOCAL_ERROR="The from vector library type of "// &
                      & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  END SELECT
                ELSE
                  !Vectors are of from different library types
                  CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)
                ENDIF
              ELSE
                LOCAL_ERROR="The from vector data type of "//TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                  & " does not match the logical data type of the supplied alpha value."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              LOCAL_ERROR="The from vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(FROM_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
                & " does not match the to vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(TO_VECTOR%DATA_TYPE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("To vector has not been finished.",ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("To vector is not associated.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("From vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("From vector is not associated.",ERR,ERROR,*999)
    ENDIF
     
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_COPY_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_COPY_L")
#endif
    RETURN 1
    
  END SUBROUTINE DISTRIBUTED_VECTOR_COPY_L

  !
  !================================================================================================================================
  !

  !>Finalise a CMISS distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_CMISS_FINALISE(CMISS_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_CMISS_TYPE), POINTER :: CMISS_VECTOR !<A pointer to the CMISS distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CMISS_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(CMISS_VECTOR)) THEN
      IF(ALLOCATED(CMISS_VECTOR%DATA_INTG)) DEALLOCATE(CMISS_VECTOR%DATA_INTG)
      IF(ALLOCATED(CMISS_VECTOR%DATA_SP)) DEALLOCATE(CMISS_VECTOR%DATA_SP)
      IF(ALLOCATED(CMISS_VECTOR%DATA_DP)) DEALLOCATE(CMISS_VECTOR%DATA_DP)
      IF(ALLOCATED(CMISS_VECTOR%DATA_L)) DEALLOCATE(CMISS_VECTOR%DATA_L)
      IF(ALLOCATED(CMISS_VECTOR%TRANSFERS)) THEN
        DO domain_idx=1,SIZE(CMISS_VECTOR%TRANSFERS)
          CALL DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE(CMISS_VECTOR,domain_idx,ERR,ERROR,*999)
        ENDDO !domain_idx
        DEALLOCATE(CMISS_VECTOR%TRANSFERS)
      ENDIF
      DEALLOCATE(CMISS_VECTOR)
    ENDIF
     
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_FINALSE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_CMISS_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CMISS_FINALISE

  !
  !================================================================================================================================
  !

  !>Intialises a CMISS distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_CMISS_INITIALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CMISS_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
        CALL FLAG_ERROR("CMISS is already associated for this distributed vector.",ERR,ERROR,*998)
      ELSE
        IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
          ALLOCATE(DISTRIBUTED_VECTOR%CMISS,STAT=ERR)
          IF(ERR/=0) CALL FLAG_ERROR("Could not allocated CMISS distributed vector.",ERR,ERROR,*999)
          DISTRIBUTED_VECTOR%CMISS%DISTRIBUTED_VECTOR=>DISTRIBUTED_VECTOR
          DISTRIBUTED_VECTOR%LIBRARY_TYPE=DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE
          !Set the defaults
          DISTRIBUTED_VECTOR%CMISS%BASE_TAG_NUMBER=0
          SELECT CASE(DISTRIBUTED_VECTOR%GHOSTING_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
            DISTRIBUTED_VECTOR%CMISS%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
          CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
            DISTRIBUTED_VECTOR%CMISS%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_LOCAL
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector ghosting type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT         
          DISTRIBUTED_VECTOR%CMISS%DATA_SIZE=0         
        ELSE
          CALL FLAG_ERROR("Distributed vector domain mapping is not associated.",ERR,ERROR,*998)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_INITIALSE")
#endif
    RETURN
999 IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) &
      & CALL DISTRIBUTED_VECTOR_CMISS_FINALISE(DISTRIBUTED_VECTOR%CMISS,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_CMISS_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CMISS_INITIALISE

  !
  !================================================================================================================================
  !

  !>Finishes the creation of a CMISS distributed vector
  SUBROUTINE DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH(CMISS_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_CMISS_TYPE), POINTER :: CMISS_VECTOR !<A pointer to the distributed CMISS vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx,domain_idx2,domain_no,DUMMY_ERR,my_computational_node_number
    LOGICAL :: FOUND
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(CMISS_VECTOR)) THEN
      DISTRIBUTED_VECTOR=>CMISS_VECTOR%DISTRIBUTED_VECTOR
      IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
        DOMAIN_MAPPING=>DISTRIBUTED_VECTOR%DOMAIN_MAPPING
        IF(ASSOCIATED(DOMAIN_MAPPING)) THEN
          CMISS_VECTOR%DATA_SIZE=CMISS_VECTOR%N
          SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            ALLOCATE(CMISS_VECTOR%DATA_INTG(CMISS_VECTOR%DATA_SIZE),STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed vector integer data.",ERR,ERROR,*999)
          CASE(MATRIX_VECTOR_SP_TYPE)
            ALLOCATE(CMISS_VECTOR%DATA_SP(CMISS_VECTOR%DATA_SIZE),STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed vector single precsion data.",ERR,ERROR,*999)
          CASE(MATRIX_VECTOR_DP_TYPE)
            ALLOCATE(CMISS_VECTOR%DATA_DP(CMISS_VECTOR%DATA_SIZE),STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed vector double precsion data.",ERR,ERROR,*999)
          CASE(MATRIX_VECTOR_L_TYPE)
            ALLOCATE(CMISS_VECTOR%DATA_L(CMISS_VECTOR%DATA_SIZE),STAT=ERR)
            IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed vector logical data.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          CMISS_VECTOR%BASE_TAG_NUMBER=DISTRIBUTED_DATA_ID
          IF(DOMAIN_MAPPING%NUMBER_OF_DOMAINS==1) THEN
            DISTRIBUTED_DATA_ID=DISTRIBUTED_DATA_ID+1
          ELSE
            DISTRIBUTED_DATA_ID=DISTRIBUTED_DATA_ID+ &
              & DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(DOMAIN_MAPPING%NUMBER_OF_DOMAINS)
          END IF
          IF(DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS>0) THEN
            my_computational_node_number=COMPUTATIONAL_NODE_NUMBER_GET(ERR,ERROR)
            IF(ERR/=0) GOTO 999
            IF(DISTRIBUTED_VECTOR%GHOSTING_TYPE==DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE) THEN
              ALLOCATE(CMISS_VECTOR%TRANSFERS(DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS),STAT=ERR)
              IF(ERR/=0) CALL FLAG_ERROR("Could not allocate CMISS distributed vector transfer buffers.",ERR,ERROR,*999)
              DO domain_idx=1,DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                CALL DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE(CMISS_VECTOR,domain_idx,ERR,ERROR,*999)
                CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE=DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                  & NUMBER_OF_SEND_GHOSTS
                CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE= &
                  & DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_RECEIVE_GHOSTS
                CMISS_VECTOR%TRANSFERS(domain_idx)%DATA_TYPE=DISTRIBUTED_VECTOR%DATA_TYPE
                CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_TAG_NUMBER=CMISS_VECTOR%BASE_TAG_NUMBER + &
                  & DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(my_computational_node_number)+domain_idx-1
                domain_no=DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER
                FOUND=.FALSE.
                DO domain_idx2=DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(domain_no),DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(domain_no+1)-1
                  IF(DOMAIN_MAPPING%ADJACENT_DOMAINS_LIST(domain_idx2)==my_computational_node_number) THEN
                    FOUND=.TRUE.
                    EXIT
                  ENDIF
                ENDDO !domain_idx2
                IF(FOUND) THEN
                  domain_idx2=domain_idx2-DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(domain_no)+1
                  CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER=CMISS_VECTOR%BASE_TAG_NUMBER + &
                    & DOMAIN_MAPPING%ADJACENT_DOMAINS_PTR(domain_no)+domain_idx2-1
                ELSE
                  CALL FLAG_ERROR("Could not find domain to set the receive tag number.",ERR,ERROR,*999)
                ENDIF
                SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
                CASE(DISTRIBUTED_MATRIX_VECTOR_INTG_TYPE)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_INTG(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & SEND_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector send integer transfer buffer.",ERR,ERROR,*999)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_INTG(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & RECEIVE_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector receive integer transfer buffer.", &
                    & ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_VECTOR_SP_TYPE)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SP(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & SEND_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector send single precision transfer buffer.", &
                    & ERR,ERROR,*999)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SP(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & RECEIVE_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector receive single precision transfer buffer.", &
                    & ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_VECTOR_DP_TYPE)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_DP(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & SEND_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector send double precision transfer buffer.", &
                    & ERR,ERROR,*999)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_DP(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & RECEIVE_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector receive double precision transfer buffer.", &
                    & ERR,ERROR,*999)
                CASE(DISTRIBUTED_MATRIX_VECTOR_L_TYPE)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_L(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE), &
                    & STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector send logical transfer buffer.",ERR,ERROR,*999)
                  ALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_L(CMISS_VECTOR%TRANSFERS(domain_idx)% &
                    & RECEIVE_BUFFER_SIZE),STAT=ERR)
                  IF(ERR/=0) CALL FLAG_ERROR("Could not allocate distributed vector receive logical transfer buffer.", &
                    & ERR,ERROR,*999)
                CASE DEFAULT
                  LOCAL_ERROR="The distributed vector data type of "// &
                    & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                  CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                END SELECT
              ENDDO !domain_idx
            ENDIF
          ENDIF
        ELSE
          CALL FLAG_ERROR("CMISS vector distributed vector domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("CMISS vector distributed vector is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("CMISS vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH")
#endif
    RETURN
999 CALL DISTRIBUTED_VECTOR_CMISS_FINALISE(CMISS_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)      
998 CALL ERRORS("DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Finishes the creation a distributed vector
  SUBROUTINE DISTRIBUTED_VECTOR_CREATE_FINISH(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CREATE_FINISH",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        CALL FLAG_ERROR("The distributed vector has already been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            CALL DISTRIBUTED_VECTOR_CMISS_CREATE_FINISH(DISTRIBUTED_VECTOR%CMISS,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            CALL DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH(DISTRIBUTED_VECTOR%PETSC,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
        DISTRIBUTED_VECTOR%VECTOR_FINISHED=.TRUE.
      ENDIF
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CREATE_FINISH")
#endif
    RETURN
999 IF(ASSOCIATED(DISTRIBUTED_VECTOR)) CALL DISTRIBUTED_VECTOR_FINALISE(DISTRIBUTED_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)
    DEALLOCATE(DISTRIBUTED_VECTOR)
998 CALL ERRORS("DISTRIBUTED_VECTOR_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Starts the creation a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_CREATE_START(DOMAIN_MAPPING,DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: DOMAIN_MAPPING !<A pointer to the domain mapping used to distribute this vector
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<On return, a pointer to the created distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CREATE_START",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DOMAIN_MAPPING)) THEN
      IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
        CALL FLAG_ERROR("Distributed vector is already associated.",ERR,ERROR,*998)
      ELSE
        CALL DISTRIBUTED_VECTOR_INITIALISE(DOMAIN_MAPPING,DISTRIBUTED_VECTOR,ERR,ERROR,*999)
        !Set the default values
      ENDIF
    ELSE
      CALL FLAG_ERROR("Domain mapping is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CREATE_START")
#endif
    RETURN
999 CALL DISTRIBUTED_VECTOR_FINALISE(DISTRIBUTED_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_CREATE_START",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CREATE_START")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CREATE_START

  !
  !================================================================================================================================
  !

  !>Gets the data type of a distributed vector.
  SUBROUTINE DistributedVector_DataTypeGet(vector,dataType,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: vector !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: dataType !<On return, the data type of the vector. \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string

    CALL enters("DistributedVector_DataTypeGet",err,error,*999)

    IF(ASSOCIATED(vector)) THEN
      IF(.NOT.vector%vector_finished) THEN
        CALL flag_error("The vector has not been finished.",err,error,*999)
      ELSE
        dataType=vector%data_type
      END IF
    ELSE
      CALL flag_error("Distributed vector is not associated.",err,error,*999)
    END IF

    CALL exits("DistributedVector_DataTypeGet")
    RETURN
999 CALL errors("DistributedVector_DataTypeGet",err,error)
    CALL exits("DistributedVector_DataTypeGet")
    RETURN 1
  END SUBROUTINE DistributedVector_DataTypeGet

  !
  !================================================================================================================================
  !

  !>Sets/changes the data type of a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_TYPE_SET(DISTRIBUTED_VECTOR,DATA_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: DATA_TYPE !<The data type to be set \see DISTRIBUTED_MATRIX_VECTOR_DataTypes,DISTRIBUTED_MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_TYPE_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        CALL FLAG_ERROR("The distributed vector has been finished.",ERR,ERROR,*999)
      ELSE
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          SELECT CASE(DATA_TYPE)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_INTG_TYPE
          CASE(MATRIX_VECTOR_SP_TYPE)
            DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_SP_TYPE
          CASE(MATRIX_VECTOR_DP_TYPE)
            DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_DP_TYPE
          CASE(MATRIX_VECTOR_L_TYPE)
            DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_L_TYPE
          CASE DEFAULT
            LOCAL_ERROR="The distributed data type of "//TRIM(NUMBER_TO_VSTRING(DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          SELECT CASE(DATA_TYPE)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            CALL FLAG_ERROR("An integer distributed PETSc vector is not implemented.",ERR,ERROR,*999)
          CASE(MATRIX_VECTOR_SP_TYPE)
            CALL FLAG_ERROR("A single precision distributed PETSc vector is not implemented.",ERR,ERROR,*999)
          CASE(MATRIX_VECTOR_DP_TYPE)
            DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_DP_TYPE
          CASE(MATRIX_VECTOR_L_TYPE)
            CALL FLAG_ERROR("A logical distributed PETSc vector is not implemented.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed data type of "//TRIM(NUMBER_TO_VSTRING(DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_TYPE_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Destroys a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_DESTROY(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DESTROY",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      CALL DISTRIBUTED_VECTOR_FINALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*999)
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DESTROY")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DESTROY",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DESTROY")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DESTROY

  !
  !================================================================================================================================
  !

  !>Duplicates the structure of a distributed vector and returns a pointer to the new distributed vector in NEW_DISTRIBUTED_VECTOR.
  SUBROUTINE DISTRIBUTED_VECTOR_DUPLICATE(DISTRIBUTED_VECTOR,NEW_DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector to duplicate
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: NEW_DISTRIBUTED_VECTOR !<On return a pointer to the new duplicated distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DUPLICATE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(NEW_DISTRIBUTED_VECTOR)) THEN
        CALL FLAG_ERROR("New distributed vector is already associated.",ERR,ERROR,*998)
      ELSE
        CALL DISTRIBUTED_VECTOR_CREATE_START(DISTRIBUTED_VECTOR%DOMAIN_MAPPING,NEW_DISTRIBUTED_VECTOR,ERR,ERROR,*999)
        CALL DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET(NEW_DISTRIBUTED_VECTOR,DISTRIBUTED_VECTOR%LIBRARY_TYPE,ERR,ERROR,*999)
        CALL DISTRIBUTED_VECTOR_DATA_TYPE_SET(NEW_DISTRIBUTED_VECTOR,DISTRIBUTED_VECTOR%DATA_TYPE,ERR,ERROR,*999)
        CALL DISTRIBUTED_VECTOR_CREATE_FINISH(NEW_DISTRIBUTED_VECTOR,ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DUPLICATE")
#endif
    RETURN
999 CALL DISTRIBUTED_VECTOR_FINALISE(NEW_DISTRIBUTED_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_DUPLICATE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DUPLICATE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DUPLICATE

  !
  !================================================================================================================================
  !

  !>Finalises a distributed vector and deallocates all memory.
  SUBROUTINE DISTRIBUTED_VECTOR_FINALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      CALL DISTRIBUTED_VECTOR_CMISS_FINALISE(DISTRIBUTED_VECTOR%CMISS,ERR,ERROR,*999)
      CALL DISTRIBUTED_VECTOR_PETSC_FINALISE(DISTRIBUTED_VECTOR%PETSC,ERR,ERROR,*999)        
      DEALLOCATE(DISTRIBUTED_VECTOR)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_FINALISE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_FINALISE

  !
  !================================================================================================================================
  !

  !>Initialises a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_INITIALISE(DOMAIN_MAPPING,DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: DOMAIN_MAPPING !<A pointer to the domain mapping used to distribute this vector
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DOMAIN_MAPPING)) THEN
      IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
        CALL FLAG_ERROR("Distributed vector is already associated.",ERR,ERROR,*998)
      ELSE
        ALLOCATE(DISTRIBUTED_VECTOR,STAT=ERR)
        IF(ERR/=0) CALL FLAG_ERROR("Could not allocated the distributed vector.",ERR,ERROR,*999)
        DISTRIBUTED_VECTOR%VECTOR_FINISHED=.FALSE.
        DISTRIBUTED_VECTOR%LIBRARY_TYPE=0
        DISTRIBUTED_VECTOR%GHOSTING_TYPE=DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE
        DISTRIBUTED_VECTOR%DOMAIN_MAPPING=>DOMAIN_MAPPING
        DISTRIBUTED_VECTOR%DATA_TYPE=MATRIX_VECTOR_DP_TYPE
        NULLIFY(DISTRIBUTED_VECTOR%CMISS)
        NULLIFY(DISTRIBUTED_VECTOR%PETSC)
        CALL DISTRIBUTED_VECTOR_CMISS_INITIALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Domain mapping is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_INITIALISE")
#endif
    RETURN
999 CALL DISTRIBUTED_VECTOR_FINALISE(DISTRIBUTED_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_INITIALISE

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of an integer distributed vector. Note: the values can be used for read operations but a DISTRIBUTED_VECTOR_VALUES_SET call must be used to change any values. The pointer should not be deallocated.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_INTG(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), POINTER :: DATA(:) !<On return, a pointer to the data of the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_GET_INTG",ERR,ERROR,*999)
#endif

     IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DATA=>DISTRIBUTED_VECTOR%CMISS%DATA_INTG
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot get data for an integer PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the integer data type of the requested values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_GET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_INTG

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of a single precision distributed vector. Note: the values can be used for read operations but a DISTRIBUTED_VECTOR_VALUES_SET call must be used to change any values. The pointer should not be deallocated.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_SP(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(SP), POINTER :: DATA(:) !<On return, a pointer to the data of the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_GET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DATA=>DISTRIBUTED_VECTOR%CMISS%DATA_SP
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the single precision data type of the requested values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_GET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_SP

  !
  !================================================================================================================================
  !

  !> Returns a pointer to the data of a double precision distributed vector. Note: the values can be used for read operations but a DISTRIBUTED_VECTOR_VALUES_SET call must be used to change any values. The pointer should not be deallocated.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_DP(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(DP), POINTER :: DATA(:) !<On return, a pointer to the data of the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_GET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DATA=>DISTRIBUTED_VECTOR%CMISS%DATA_DP
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
                IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                  CALL PETSC_VECGETARRAYF90(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,DATA,ERR,ERROR,*999)
                ELSE
                  CALL PETSC_VECGETARRAYF90(DISTRIBUTED_VECTOR%PETSC%VECTOR,DATA,ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the double precision data type of the requested values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_GET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_DP

  !
  !================================================================================================================================
  !

  !>Returns a pointer to the data of a logical distributed vector. Note: the values can be used for read operations but a DISTRIBUTED_VECTOR_VALUES_SET call must be used to change any values. The pointer should not be deallocated.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_L(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    LOGICAL, POINTER :: DATA(:) !<On return, a pointer to the data of the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_GET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        CALL FLAG_ERROR("Data is already associated.",ERR,ERROR,*999)
      ELSE
        NULLIFY(DATA)
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DATA=>DISTRIBUTED_VECTOR%CMISS%DATA_L
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot get values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "//TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the logical data type of the requested values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_GET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_GET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_GET_L

  !
  !================================================================================================================================
  !

  !>Restores the integer data pointer returned from DISTRIBUTED_VECTOR_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_INTG(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), POINTER :: DATA(:) !<The a pointer to the distributed vector data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_RESTORE_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)              
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for an integer PETSc distributed vector.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_RESTORE_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_INTG

  !
  !================================================================================================================================
  !

  !>Restores the single precision data pointer returned from DISTRIBUTED_VECTOR_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_SP(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(SP), POINTER :: DATA(:) !<A pointer to the distributed vector data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_RESTORE_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for a single precision PETSc distributed vector.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_RESTORE_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_SP

  !
  !================================================================================================================================
  !

  !>Restores the double precision data pointer returned from DISTRIBUTED_VECTOR_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_DP(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    REAL(DP), POINTER :: DATA(:) !<A pointer to the distributed vector data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_RESTORE_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                CALL PETSC_VECRESTOREARRAYF90(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,DATA,ERR,ERROR,*999)
              ELSE
                CALL PETSC_VECRESTOREARRAYF90(DISTRIBUTED_VECTOR%PETSC%VECTOR,DATA,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_RESTORE_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_DP

  !
  !================================================================================================================================
  !

  !>Restores the logical data pointer returned from DISTRIBUTED_VECTOR_DATA_GET once the data has finished being used.
  SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_L(DISTRIBUTED_VECTOR,DATA,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    LOGICAL, POINTER :: DATA(:) !<A pointer to the distributed vector data for this computational node
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
 
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_DATA_RESTORE_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DATA)) THEN
        IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            NULLIFY(DATA)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot restore data for a logical PETSc distributed vector.",ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed matrix library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("Data is not associated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_DATA_RESTORE_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_DATA_RESTORE_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_DATA_RESTORE_L
  
  !
  !================================================================================================================================
  !

  !>Sets/changes the ghosting type for a distributed vector
  SUBROUTINE DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET(DISTRIBUTED_VECTOR,GHOSTING_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector 
    INTEGER(INTG), INTENT(IN) :: GHOSTING_TYPE !<The ghosting type \see DISTRIBUTED_MATRIX_VECTOR_GhostingTypes,DISTRIBUTED_MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        CALL FLAG_ERROR("The distributed vector has already been finished.",ERR,ERROR,*999)
      ELSE
        IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              SELECT CASE(GHOSTING_TYPE)
              CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
                DISTRIBUTED_VECTOR%CMISS%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
              CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
                DISTRIBUTED_VECTOR%CMISS%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_LOCAL
              CASE DEFAULT
                LOCAL_ERROR="The given ghosting type of "//TRIM(NUMBER_TO_VSTRING(GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              END SELECT
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              SELECT CASE(GHOSTING_TYPE)
              CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
                DISTRIBUTED_VECTOR%PETSC%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
              CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
                DISTRIBUTED_VECTOR%PETSC%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_LOCAL
              CASE DEFAULT
                LOCAL_ERROR="The given ghosting type of "//TRIM(NUMBER_TO_VSTRING(GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              END SELECT              
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          DISTRIBUTED_VECTOR%GHOSTING_TYPE=GHOSTING_TYPE
        ELSE
          CALL FLAG_ERROR("Distributed vector domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_GHOSTING_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Sets/changes the library type for a distributed vector
  SUBROUTINE DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET(DISTRIBUTED_VECTOR,LIBRARY_TYPE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector 
    INTEGER(INTG), INTENT(IN) :: LIBRARY_TYPE !<The library type \see DISTRIBUTED_MATRIX_VECTOR_LibraryTypes,DISTRIBUTED_MATRIX_VECTOR
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR,OLD_LIBRARY_TYPE
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        CALL FLAG_ERROR("The distributed vector has already been finished.",ERR,ERROR,*998)
      ELSE
        OLD_LIBRARY_TYPE=DISTRIBUTED_VECTOR%LIBRARY_TYPE
        IF(LIBRARY_TYPE/=OLD_LIBRARY_TYPE) THEN
          !Initialise the new library type
          SELECT CASE(LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            CALL DISTRIBUTED_VECTOR_CMISS_INITIALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL DISTRIBUTED_VECTOR_PETSC_INITIALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "//TRIM(NUMBER_TO_VSTRING(LIBRARY_TYPE,"*",ERR,ERROR))// &
              & " is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          !Finalise the old library type
          SELECT CASE(OLD_LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            CALL DISTRIBUTED_VECTOR_CMISS_FINALISE(DISTRIBUTED_VECTOR%CMISS,ERR,ERROR,*999)
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL DISTRIBUTED_VECTOR_PETSC_FINALISE(DISTRIBUTED_VECTOR%PETSC,ERR,ERROR,*999)
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(OLD_LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          DISTRIBUTED_VECTOR%LIBRARY_TYPE=LIBRARY_TYPE
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET")
#endif
    RETURN
999 SELECT CASE(LIBRARY_TYPE)
    CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
      CALL DISTRIBUTED_VECTOR_CMISS_FINALISE(DISTRIBUTED_VECTOR%CMISS,DUMMY_ERR,DUMMY_ERROR,*998)
    CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
      CALL DISTRIBUTED_VECTOR_PETSC_FINALISE(DISTRIBUTED_VECTOR%PETSC,DUMMY_ERR,DUMMY_ERROR,*998)
    END SELECT
998 CALL ERRORS("DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_LIBRARY_TYPE_SET

  !
  !================================================================================================================================
  !

  !>Outputs a distributed vector to the specified output ID.
  SUBROUTINE DISTRIBUTED_VECTOR_OUTPUT(ID,DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    INTEGER(INTG), INTENT(IN) :: ID !<The ID of the output stream
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector to duplicate
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    REAL(DP), POINTER :: VECTOR(:)
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_OUTPUT",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
            CASE(MATRIX_VECTOR_INTG_TYPE)
              CALL WRITE_STRING_VECTOR(ID,1,1,DISTRIBUTED_VECTOR%CMISS%N,8,8,DISTRIBUTED_VECTOR%CMISS%DATA_INTG, &
                & '("Vector(:)          :",8(X,I13))','(20X,8(X,I13))',ERR,ERROR,*999)
            CASE(MATRIX_VECTOR_SP_TYPE)
              CALL WRITE_STRING_VECTOR(ID,1,1,DISTRIBUTED_VECTOR%CMISS%N,8,8,DISTRIBUTED_VECTOR%CMISS%DATA_SP, &
                & '("Vector(:)          :",8(X,E13.6))','(20X,8(X,E13.6))',ERR,ERROR,*999)
            CASE(MATRIX_VECTOR_DP_TYPE)
              CALL WRITE_STRING_VECTOR(ID,1,1,DISTRIBUTED_VECTOR%CMISS%N,8,8,DISTRIBUTED_VECTOR%CMISS%DATA_DP, &
                & '("Vector(:)          :",8(X,E13.6))','(20X,8(X,E13.6))',ERR,ERROR,*999)
            CASE(MATRIX_VECTOR_L_TYPE)            
              CALL WRITE_STRING_VECTOR(ID,1,1,DISTRIBUTED_VECTOR%CMISS%N,8,8,DISTRIBUTED_VECTOR%CMISS%DATA_INTG, &
                & '("Vector(:)          :",8(X,L13))','(20X,8(X,L13))',ERR,ERROR,*999)
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector data type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            NULLIFY(VECTOR)
            IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
              CALL PETSC_VECGETARRAYF90(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,VECTOR,ERR,ERROR,*999)
            ELSE              
              CALL PETSC_VECGETARRAYF90(DISTRIBUTED_VECTOR%PETSC%VECTOR,VECTOR,ERR,ERROR,*999)
            ENDIF
            CALL WRITE_STRING_VECTOR(ID,1,1,DISTRIBUTED_VECTOR%PETSC%N,8,8,VECTOR, &
              & '("Vector(:)          :",8(X,E13.6))','(20X,8(X,E13.6))',ERR,ERROR,*999)
            IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
              CALL PETSC_VECRESTOREARRAYF90(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,VECTOR,ERR,ERROR,*999)
            ELSE              
              CALL PETSC_VECRESTOREARRAYF90(DISTRIBUTED_VECTOR%PETSC%VECTOR,VECTOR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
       CALL FLAG_ERROR("Distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OUTPUT")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_OUTPUT",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OUTPUT")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_OUTPUT

  !
  !================================================================================================================================
  !

  !>Sets the override vector for a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_OVERRIDE_SET_ON(DISTRIBUTED_VECTOR,OVERRIDE_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector to override
    TYPE(PETSC_VEC_TYPE), INTENT(IN) :: OVERRIDE_VECTOR !<The override vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_OVERRIDE_SET_ON",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)          
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR=.TRUE.
            DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR=OVERRIDE_VECTOR
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
       CALL FLAG_ERROR("Distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OVERRIDE_SET_ON")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_OVERRIDE_SET_ON",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OVERRIDE_SET_ON")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_OVERRIDE_SET_ON

  !
  !================================================================================================================================
  !

  !>Turns off the override vector for a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector to override
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          CALL FLAG_ERROR("Not implemented.",ERR,ERROR,*999)          
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR=.FALSE.
            CALL PETSC_VECINITIALISE(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
       CALL FLAG_ERROR("Distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_OVERRIDE_SET_OFF

  !
  !================================================================================================================================
  !

  !>Finishes the creation of a PETSc distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH(PETSC_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_PETSC_TYPE), POINTER :: PETSC_VECTOR !<A pointer to the distributed PETSc vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR,i
    INTEGER(INTG), ALLOCATABLE :: GLOBAL_NUMBERS(:)
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR
    TYPE(DOMAIN_MAPPING_TYPE), POINTER :: DOMAIN_MAPPING
    TYPE(VARYING_STRING) :: DUMMY_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(PETSC_VECTOR)) THEN
      DISTRIBUTED_VECTOR=>PETSC_VECTOR%DISTRIBUTED_VECTOR
      IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
        DOMAIN_MAPPING=>DISTRIBUTED_VECTOR%DOMAIN_MAPPING
        IF(ASSOCIATED(DOMAIN_MAPPING)) THEN
          !Create the PETSc vector
          PETSC_VECTOR%DATA_SIZE=PETSC_VECTOR%N
          CALL PETSC_VECCREATEMPI(COMPUTATIONAL_ENVIRONMENT%MPI_COMM,PETSC_VECTOR%N,PETSC_VECTOR%GLOBAL_N,PETSC_VECTOR%VECTOR, &
            & ERR,ERROR,*999)
          !Set up the Local to Global Mappings
          DO i=1,PETSC_VECTOR%N
            PETSC_VECTOR%GLOBAL_NUMBERS(i)=DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(i)-1
          ENDDO !i
        ELSE
          CALL FLAG_ERROR("PETSc vector distributed vector domain mapping is not associated.",ERR,ERROR,*999)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("PETSc vector is not associated.",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH")
#endif
    RETURN
999 IF(ALLOCATED(GLOBAL_NUMBERS)) DEALLOCATE(GLOBAL_NUMBERS)
    CALL DISTRIBUTED_VECTOR_PETSC_FINALISE(PETSC_VECTOR,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_PETSC_CREATE_FINISH

  !
  !================================================================================================================================
  !

  !>Finalise a PETSc distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_PETSC_FINALISE(PETSC_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_PETSC_TYPE), POINTER :: PETSC_VECTOR !<A pointer to the PETSc distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_PETSC_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(PETSC_VECTOR)) THEN
      IF(ALLOCATED(PETSC_VECTOR%GLOBAL_NUMBERS)) DEALLOCATE(PETSC_VECTOR%GLOBAL_NUMBERS)
      CALL PETSC_VECFINALISE(PETSC_VECTOR%VECTOR,ERR,ERROR,*999)
      CALL PETSC_VECFINALISE(PETSC_VECTOR%OVERRIDE_VECTOR,ERR,ERROR,*999)
      DEALLOCATE(PETSC_VECTOR)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_FINALSE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_PETSC_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_PETSC_FINALISE

  !
  !================================================================================================================================
  !

  !>Intialises a PETSc distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_PETSC_INITIALISE(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: DUMMY_ERR
    TYPE(VARYING_STRING) :: DUMMY_ERROR,LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_PETSC_INITIALISE",ERR,ERROR,*998)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
        CALL FLAG_ERROR("PETSc is already associated for this distributed vector.",ERR,ERROR,*998)
      ELSE
        IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
          ALLOCATE(DISTRIBUTED_VECTOR%PETSC,STAT=ERR)
          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc distributed vector.",ERR,ERROR,*999)
          DISTRIBUTED_VECTOR%PETSC%DISTRIBUTED_VECTOR=>DISTRIBUTED_VECTOR
          DISTRIBUTED_VECTOR%LIBRARY_TYPE=DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE
          !Set the defaults
          SELECT CASE(DISTRIBUTED_VECTOR%GHOSTING_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_INCLUDE_GHOSTS_TYPE)
            DISTRIBUTED_VECTOR%PETSC%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%TOTAL_NUMBER_OF_LOCAL
          CASE(DISTRIBUTED_MATRIX_VECTOR_NO_GHOSTS_TYPE)
            DISTRIBUTED_VECTOR%PETSC%N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_LOCAL
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector ghosting type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%GHOSTING_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
          DISTRIBUTED_VECTOR%PETSC%GLOBAL_N=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_GLOBAL
          ALLOCATE(DISTRIBUTED_VECTOR%PETSC%GLOBAL_NUMBERS(DISTRIBUTED_VECTOR%PETSC%N),STAT=ERR)
          IF(ERR/=0) CALL FLAG_ERROR("Could not allocate PETSc distributed vector global numbers.",ERR,ERROR,*999)
          DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR=.FALSE.
          CALL PETSC_VECINITIALISE(DISTRIBUTED_VECTOR%PETSC%VECTOR,ERR,ERROR,*999)
          CALL PETSC_VECINITIALISE(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)          
        ELSE
          CALL FLAG_ERROR("Distributed vector domain mapping is not associated",ERR,ERROR,*998)
        ENDIF
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated",ERR,ERROR,*998)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_INITIALSE")
#endif
    RETURN
999 IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) &
      & CALL DISTRIBUTED_VECTOR_PETSC_FINALISE(DISTRIBUTED_VECTOR%PETSC,DUMMY_ERR,DUMMY_ERROR,*998)
998 CALL ERRORS("DISTRIBUTED_VECTOR_PETSC_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_PETSC_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_PETSC_INITIALISE

  !
  !================================================================================================================================
  !

  !>Finalises a CMISS distributed vector transfer information and deallocates all memory.
  SUBROUTINE DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE(CMISS_VECTOR,domain_idx,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_CMISS_TYPE), POINTER :: CMISS_VECTOR !<A pointer to the CMISS distributed vector
    INTEGER(INTG), INTENT(IN) :: domain_idx !<The domain index of the distributed vector to finalise
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(CMISS_VECTOR)) THEN
      IF(ALLOCATED(CMISS_VECTOR%TRANSFERS)) THEN
        IF(domain_idx>0.AND.domain_idx<=SIZE(CMISS_VECTOR%TRANSFERS,1)) THEN
          NULLIFY(CMISS_VECTOR%TRANSFERS(domain_idx)%CMISS_VECTOR)
          CMISS_VECTOR%TRANSFERS(domain_idx)%DATA_TYPE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER=-1
          CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_TAG_NUMBER=-1
          CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%MPI_SEND_REQUEST=MPI_REQUEST_NULL
          CMISS_VECTOR%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST=MPI_REQUEST_NULL
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_INTG)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_INTG)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SP)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SP)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_DP)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_DP)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_L)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_L)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_INTG)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_INTG)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SP)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SP)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_DP)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_DP)
          IF(ALLOCATED(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_L)) &
            & DEALLOCATE(CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_L)
        ELSE
          LOCAL_ERROR="The domain index of "//TRIM(NUMBER_TO_VSTRING(domain_idx,"*",ERR,ERROR))// &
            & " is invalid. It must be between 1 and "//TRIM(NUMBER_TO_VSTRING(SIZE(CMISS_VECTOR%TRANSFERS,1),"*",ERR,ERROR))//"."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ENDIF
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CMISS_TRANSFER_FINALISE

  !
  !================================================================================================================================
  !

  !>Initialises a CMISS distributed vector transfer information.
  SUBROUTINE DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE(CMISS_VECTOR,domain_idx,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_CMISS_TYPE), POINTER :: CMISS_VECTOR !<A pointer to the CMISS distributed vector
    INTEGER(INTG), INTENT(IN) :: domain_idx !<The domain index to initialise
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR

#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(CMISS_VECTOR)) THEN
      IF(ALLOCATED(CMISS_VECTOR%TRANSFERS)) THEN
        IF(domain_idx>0.AND.domain_idx<=SIZE(CMISS_VECTOR%TRANSFERS,1)) THEN
          CMISS_VECTOR%TRANSFERS(domain_idx)%CMISS_VECTOR=>CMISS_VECTOR
          CMISS_VECTOR%TRANSFERS(domain_idx)%DATA_TYPE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE=0
          CMISS_VECTOR%TRANSFERS(domain_idx)%SEND_TAG_NUMBER=-1
          CMISS_VECTOR%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER=-1
          CMISS_VECTOR%TRANSFERS(domain_idx)%MPI_SEND_REQUEST=MPI_REQUEST_NULL
          CMISS_VECTOR%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST=MPI_REQUEST_NULL
        ELSE
          LOCAL_ERROR="The domain index of "//TRIM(NUMBER_TO_VSTRING(domain_idx,"*",ERR,ERROR))// &
            & " is invalid. It must be between 1 and "// &
            & TRIM(NUMBER_TO_VSTRING(SIZE(CMISS_VECTOR%TRANSFERS,1),"*",ERR,ERROR))//"."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("CMISS vector transfers is not allocated.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("CMISS vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_CMISS_TRANSFER_INITIALISE

  !
  !================================================================================================================================
  !

  !>Finishes the (ghost) update procedure for a distributed vector. This routine will wait until all transfers have completed!
  SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_FINISH(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx,i,NUMBER_OF_COMPUTATIONAL_NODES
    TYPE(VARYING_STRING) :: LOCAL_ERROR
   
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_UPDATE_FINISH",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
              NUMBER_OF_COMPUTATIONAL_NODES=COMPUTATIONAL_NODES_NUMBER_GET(ERR,ERROR)
              IF(ERR/=0) GOTO 999
              IF(NUMBER_OF_COMPUTATIONAL_NODES>1) THEN
                CALL DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED(DISTRIBUTED_VECTOR,ERR,ERROR,*999)
                !Copy the receive buffers back to the ghost positions in the data vector
                DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                  SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
                  CASE(MATRIX_VECTOR_INTG_TYPE)
                    DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_RECEIVE_GHOSTS
                      DISTRIBUTED_VECTOR%CMISS%DATA_INTG(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                        & LOCAL_GHOST_RECEIVE_INDICES(i))=DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_INTG(i)
                    ENDDO !i
                  CASE(MATRIX_VECTOR_SP_TYPE)
                    DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_RECEIVE_GHOSTS
                      DISTRIBUTED_VECTOR%CMISS%DATA_SP(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                        & LOCAL_GHOST_RECEIVE_INDICES(i))=DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SP(i)
                    ENDDO !i
                  CASE(MATRIX_VECTOR_DP_TYPE)
                    DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_RECEIVE_GHOSTS
                      DISTRIBUTED_VECTOR%CMISS%DATA_DP(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                        & LOCAL_GHOST_RECEIVE_INDICES(i))=DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_DP(i)
                    ENDDO !i
                  CASE(MATRIX_VECTOR_L_TYPE)
                    DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_RECEIVE_GHOSTS
                      DISTRIBUTED_VECTOR%CMISS%DATA_L(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                        & LOCAL_GHOST_RECEIVE_INDICES(i))=DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_L(i)
                    ENDDO !i
                  CASE DEFAULT
                    LOCAL_ERROR="The distributed vector data type of "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  END SELECT
                ENDDO !domain_idx
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector domain mapping is not associated.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
              CALL PETSC_VECASSEMBLYEND(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)
            ELSE
              CALL PETSC_VECASSEMBLYEND(DISTRIBUTED_VECTOR%PETSC%VECTOR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF

    IF(DIAGNOSTICS1) THEN
      SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
      CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
        IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
          CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"Distributed vector :",ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Data type = ",DISTRIBUTED_VECTOR%DATA_TYPE,ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Base tag number = ",DISTRIBUTED_VECTOR%CMISS%BASE_TAG_NUMBER, &
            & ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Number of adjacent domains = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
            & NUMBER_OF_ADJACENT_DOMAINS,ERR,ERROR,*999)
          DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Domain idx = ",domain_idx,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Domain number = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
              & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Receive tag number = ",DISTRIBUTED_VECTOR%CMISS% &
              & TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Send tag number = ",DISTRIBUTED_VECTOR%CMISS% &
              & TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    MPI send request = ",DISTRIBUTED_VECTOR%CMISS% &
              & TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    MPI receive request = ",DISTRIBUTED_VECTOR%CMISS% &
              & TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)
          ENDDO !domain_idx
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Data size = ",DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,ERR,ERROR,*999)
          SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_INTG,'("  Data :",5(X,I13))','(8X,5(X,I13))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_SP_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_SP,'("  Data :",5(X,E13.6))','(8X,5(X,E13.6))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_DP_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_DP,'("  Data :",5(X,E13.6))','(8X,5(X,E13.6))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_L_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,8,8,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_L,'("  Data :",8(X,L))','(8X,8(X,L))',ERR,ERROR,*999)      
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
        ENDIF
      CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
        !Do nothing
      CASE DEFAULT
        LOCAL_ERROR="The distributed vector library type of "// &
          & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
        CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
      END SELECT
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_FINISH")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_UPDATE_FINISH",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_FINISH")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_FINISH

  !
  !================================================================================================================================
  !

  !>Tests to see if a distributed vector update has finised! \todo USE MPI_TESTALL and store the request handles as big array.
  SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_ISFINISHED(DISTRIBUTED_VECTOR,ISFINISHED,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    LOGICAL, INTENT(OUT) :: ISFINISHED !<On return, is .TRUE. if all the transfer operations for the distributed vector have completed, .FALSE. if not
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx
    INTEGER(INTG) :: MPI_IERROR,STATUS(MPI_STATUS_SIZE)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_UPDATE_ISFINISHED",ERR,ERROR,*999)
#endif

    ISFINISHED=.FALSE.
    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
!!TODO: USE MPI_TESTALL and store the request handles as big array.
              DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                CALL MPI_TEST(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ISFINISHED,STATUS,MPI_IERROR)
                CALL MPI_ERROR_CHECK("MPI_TEST",MPI_IERROR,ERR,ERROR,*999)
                IF(.NOT.ISFINISHED) EXIT
                !CALL MPI_TEST(DISTRIBUTED_VECTOR%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ISFINISHED,STATUS,MPI_IERROR)
                !CALL MPI_ERROR_CHECK("MPI_TEST",MPI_IERROR,ERR,ERROR,*999)
                !IF(.NOT.ISFINISHED) EXIT
              ENDDO !domain_idx
            ELSE
              CALL FLAG_ERROR("Distributed vector domain mapping is not associated.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot test if update isfinished for a PETSc distributed vector.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_ISFINISHED")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_UPDATE_ISFINISHED",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_ISFINISHED")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_ISFINISHED

  !
  !================================================================================================================================
  !

  !> Waits until a distributed vector update has finised
  SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx
    INTEGER(INTG) :: MPI_IERROR,STATUS(MPI_STATUS_SIZE)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
!!TODO: USE MPI_WAITALL and store the request handles as big array.
              DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                CALL MPI_WAIT(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,STATUS,MPI_IERROR)
                CALL MPI_ERROR_CHECK("MPI_WAIT",MPI_IERROR,ERR,ERROR,*999)
              ENDDO !domain_idx
            ELSE
              CALL FLAG_ERROR("Distributed vector domain mapping is not associated.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot wait for finished for a PETSc distributed vector.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_WAITFINISHED

  !
  !================================================================================================================================
  !

  !>Starts the (ghost) update procedure for a distributed vector.
  SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_START(DISTRIBUTED_VECTOR,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: domain_idx,i,MPI_IERROR,NUMBER_OF_COMPUTATIONAL_NODES
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_UPDATE_START",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%DOMAIN_MAPPING)) THEN
              NUMBER_OF_COMPUTATIONAL_NODES=COMPUTATIONAL_NODES_NUMBER_GET(ERR,ERROR)
              IF(ERR/=0) GOTO 999
              IF(NUMBER_OF_COMPUTATIONAL_NODES>1) THEN
                IF(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS>0) THEN
                  !Fill in the send buffers with the send ghost values
                  DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                    SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
                    CASE(MATRIX_VECTOR_INTG_TYPE)
                      DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_SEND_GHOSTS
                        DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_INTG(i)= &
                          & DISTRIBUTED_VECTOR%CMISS%DATA_INTG(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                          & LOCAL_GHOST_SEND_INDICES(i))
                      ENDDO !i
                    CASE(MATRIX_VECTOR_SP_TYPE)
                      DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_SEND_GHOSTS
                        DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SP(i)= &
                          & DISTRIBUTED_VECTOR%CMISS%DATA_SP(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                          & LOCAL_GHOST_SEND_INDICES(i))
                      ENDDO !i
                    CASE(MATRIX_VECTOR_DP_TYPE)
                      DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_SEND_GHOSTS
                        DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_DP(i)= &
                          & DISTRIBUTED_VECTOR%CMISS%DATA_DP(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                          & LOCAL_GHOST_SEND_INDICES(i))
                      ENDDO !i
                    CASE(MATRIX_VECTOR_L_TYPE)
                      DO i=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%NUMBER_OF_SEND_GHOSTS
                        DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_L(i)= &
                          & DISTRIBUTED_VECTOR%CMISS%DATA_L(DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)% &
                          & LOCAL_GHOST_SEND_INDICES(i))
                      ENDDO !i
                    CASE DEFAULT
                      LOCAL_ERROR="The distributed vector data type of "// &
                        & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                      CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                    END SELECT
                  ENDDO !domain_idx
                  !Post all the receive calls first and then the send calls.
                  DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                    SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
                    CASE(MATRIX_VECTOR_INTG_TYPE)
                      CALL MPI_IRECV(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_INTG, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,MPI_INTEGER, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_IRECV",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI IRECV call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive datatype = ",MPI_INTEGER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive source = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_SP_TYPE)
                      CALL MPI_IRECV(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SP, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,MPI_REAL, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_IRECV",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI IRECV call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive datatype = ",MPI_REAL,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive source = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_DP_TYPE)
                      CALL MPI_IRECV(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_DP, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,MPI_DOUBLE_PRECISION, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_IRECV",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI IRECV call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive datatype = ",MPI_DOUBLE_PRECISION,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive source = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_L_TYPE)
                      CALL MPI_IRECV(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_L, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,MPI_LOGICAL, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_IRECV",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI IRECV call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive datatype = ",MPI_LOGICAL,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive source = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Receive request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE DEFAULT
                      LOCAL_ERROR="The distributed vector data type of "// &
                        & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                      CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                    END SELECT
                  ENDDO !domain_idx
                  !Post all the send calls.
                  DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
                    SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
                    CASE(MATRIX_VECTOR_INTG_TYPE)
                      CALL MPI_ISEND(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_INTG, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,MPI_INTEGER, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_ISEND",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI ISEND call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send datatype = ",MPI_INTEGER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send dest = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_SP_TYPE)
                      CALL MPI_ISEND(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SP, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,MPI_REAL, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_ISEND",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI ISEND call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send datatype = ",MPI_REAL,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send dest = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_DP_TYPE)
                      CALL MPI_ISEND(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_DP, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,MPI_DOUBLE_PRECISION, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_ISEND",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI ISEND call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send datatype = ",MPI_DOUBLE_PRECISION,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send dest = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE(MATRIX_VECTOR_L_TYPE)
                      CALL MPI_ISEND(DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_L, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,MPI_LOGICAL, &
                        & DISTRIBUTED_VECTOR%DOMAIN_MAPPING%ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                        & DISTRIBUTED_VECTOR%CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,MPI_IERROR)
                      CALL MPI_ERROR_CHECK("MPI_ISEND",MPI_IERROR,ERR,ERROR,*999)
                      IF(DIAGNOSTICS5) THEN
                        CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"MPI ISEND call posted:",ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send count = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_BUFFER_SIZE,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send datatype = ",MPI_LOGICAL,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send dest = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
                          & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send tag = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send comm = ",COMPUTATIONAL_ENVIRONMENT%MPI_COMM, &
                          & ERR,ERROR,*999)
                        CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Send request = ",DISTRIBUTED_VECTOR% &
                          & CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)                
                      ENDIF
                    CASE DEFAULT
                      LOCAL_ERROR="The distributed vector data type of "// &
                        & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
                      CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                    END SELECT
                  ENDDO !domain_idx
                ENDIF
              ENDIF
            ELSE
              CALL FLAG_ERROR("Domain mapping is not associated for the distributed vector.",ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)          
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
            IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
              CALL PETSC_VECASSEMBLYBEGIN(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,ERR,ERROR,*999)
            ELSE
              CALL PETSC_VECASSEMBLYBEGIN(DISTRIBUTED_VECTOR%PETSC%VECTOR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
    IF(DIAGNOSTICS1) THEN
      SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
      CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
        IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
          CALL WRITE_STRING(DIAGNOSTIC_OUTPUT_TYPE,"Distributed vector :",ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Data type = ",DISTRIBUTED_VECTOR%DATA_TYPE,ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Base tag number = ",DISTRIBUTED_VECTOR%CMISS%BASE_TAG_NUMBER, &
            & ERR,ERROR,*999)
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Number of adjacent domains = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
            & NUMBER_OF_ADJACENT_DOMAINS,ERR,ERROR,*999)
          DO domain_idx=1,DISTRIBUTED_VECTOR%DOMAIN_MAPPING%NUMBER_OF_ADJACENT_DOMAINS
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Domain idx = ",domain_idx,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Domain number = ",DISTRIBUTED_VECTOR%DOMAIN_MAPPING% &
              & ADJACENT_DOMAINS(domain_idx)%DOMAIN_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Receive tag number = ",DISTRIBUTED_VECTOR% &
              & CMISS%TRANSFERS(domain_idx)%RECEIVE_TAG_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    Send tag number = ",DISTRIBUTED_VECTOR% &
              & CMISS%TRANSFERS(domain_idx)%SEND_TAG_NUMBER,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    MPI send request = ",DISTRIBUTED_VECTOR% &
              & CMISS%TRANSFERS(domain_idx)%MPI_SEND_REQUEST,ERR,ERROR,*999)
            CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"    MPI receive request = ",DISTRIBUTED_VECTOR% &
              & CMISS%TRANSFERS(domain_idx)%MPI_RECEIVE_REQUEST,ERR,ERROR,*999)
          ENDDO !domain_idx
          CALL WRITE_STRING_VALUE(DIAGNOSTIC_OUTPUT_TYPE,"  Data size = ",DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,ERR,ERROR,*999)
          SELECT CASE(DISTRIBUTED_VECTOR%DATA_TYPE)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_INTG,'("  Data :",5(X,I13))','(8X,5(X,I13))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_SP_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_SP,'("  Data :",5(X,E13.6))','(8X,5(X,E13.6))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_DP_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,5,5,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_DP,'("  Data :",5(X,E13.6))','(8X,5(X,E13.6))',ERR,ERROR,*999)      
          CASE(MATRIX_VECTOR_L_TYPE)
            CALL WRITE_STRING_VECTOR(DIAGNOSTIC_OUTPUT_TYPE,1,1,DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,8,8,DISTRIBUTED_VECTOR%CMISS% &
              & DATA_L,'("  Data :",8(X,L))','(8X,8(X,L))',ERR,ERROR,*999)      
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
        ENDIF
      CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
        !Do nothing
      CASE DEFAULT
        LOCAL_ERROR="The distributed vector library type of "// &
          & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
        CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
      END SELECT
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_START")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_UPDATE_START",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_UPDATE_START")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_UPDATE_START

  !
  !================================================================================================================================
  !

  !>Calculates the L2 norm of a distributed vector values on this computational node
  SUBROUTINE DistributedVector_L2Norm(distributedVector,norm,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVector !<A pointer to the distributed vector
    REAL(DP), INTENT(OUT) :: norm !<The L2 norm of values from this computational node
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: localError

#if DEBUG
    CALL ENTERS("DistributedVector_L2Norm",err,error,*999)
#endif

    IF(ASSOCIATED(distributedVector)) THEN
      IF(distributedVector%VECTOR_FINISHED) THEN
        SELECT CASE(distributedVector%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          SELECT CASE(distributedVector%DATA_TYPE)
          CASE(MATRIX_VECTOR_DP_TYPE)
              IF(ASSOCIATED(distributedVector%CMISS)) THEN
                norm=0.0_DP
                DO i=1,distributedVector%CMISS%DATA_SIZE
                  norm=norm+(distributedVector%CMISS%DATA_DP(i)**2)
                ENDDO !i
                norm=SQRT(norm)
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",err,error,*999)
              ENDIF
          CASE(MATRIX_VECTOR_SP_TYPE)
            CALL FLAG_ERROR("Not implemented.",err,error,*999)
          CASE(MATRIX_VECTOR_INTG_TYPE)
            CALL FLAG_ERROR("Not implemented.",err,error,*999)
          CASE(MATRIX_VECTOR_L_TYPE)
            CALL FLAG_ERROR("Not implemented.",err,error,*999)
          CASE DEFAULT
            localError="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(distributedVector%DATA_TYPE,"*",err,error))// &
              & " is invalid."
            CALL FLAG_ERROR(localError,err,error,*999)
          END SELECT
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot calculate norm for a PETSc distributed vector.",err,error,*999)
        CASE DEFAULT
          localError="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(distributedVector%LIBRARY_TYPE,"*",err,error))//" is invalid."
          CALL FLAG_ERROR(localError,err,error,*999)
        END SELECT
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",err,error,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",err,error,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DistributedVector_L2Norm")
#endif
    RETURN
999 CALL ERRORS("DistributedVector_L2Norm",err,error)
#if DEBUG
    CALL EXITS("DistributedVector_L2Norm")
#endif
    RETURN 1
  END SUBROUTINE DistributedVector_L2Norm
  
  !
  !================================================================================================================================
  !

  !>Calculates the dot product of 2 distributed integer vectors on this computational node
  SUBROUTINE DistributedVector_VecDotIntg(distributedVectorA,distributedVectorB,dotProduct,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorA !<A pointer to the distributed vector A
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorB !<A pointer to the distributed vector B
    INTEGER(INTG), INTENT(OUT) :: dotProduct !<The dot product on this computational node
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string
    !Local Variables
    INTEGER(INTG) :: dataTypeA,dataTypeB,i
    TYPE(VARYING_STRING) :: localError

#if DEBUG
    CALL ENTERS("DistributedVector_VecDotIntg",err,error,*999)
#endif

    IF(ASSOCIATED(distributedVectorA) .AND. ASSOCIATED(distributedVectorB)) THEN
      IF(distributedVectorA%VECTOR_FINISHED .AND. distributedVectorB%VECTOR_FINISHED) THEN
        IF (distributedVectorA%LIBRARY_TYPE==distributedVectorB%LIBRARY_TYPE) THEN
          CALL DistributedVector_DataTypeGet(distributedVectorA,dataTypeA,err,error,*999)
          CALL DistributedVector_DataTypeGet(distributedVectorB,dataTypeB,err,error,*999)
          IF(dataTypeA==dataTypeB) THEN
            SELECT CASE(distributedVectorA%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(distributedVectorA%CMISS)) THEN
                IF(distributedVectorA%CMISS%DATA_SIZE==distributedVectorB%CMISS%DATA_SIZE) THEN
                  IF(distributedVectorA%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
                    dotProduct=0
                    DO i=1,distributedVectorA%CMISS%DATA_SIZE
                      dotProduct=dotProduct+(distributedVectorA%CMISS%DATA_INTG(i)*distributedVectorB%CMISS%DATA_INTG(i))
                    ENDDO !i
                  ELSE
                    CALL FLAG_ERROR("Input distributed vector data type does not match output.",err,error,*999)
                  ENDIF
                ELSE
                  CALL FLAG_ERROR("The distributed vectors do not have the same size.",err,error,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",err,error,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Distributed vector PETSC is double-precision, output scalar should be DP",err,error,*999)
            CASE DEFAULT
              localError="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(distributedVectorA%LIBRARY_TYPE,"*",err,error))//" is invalid."
              CALL FLAG_ERROR(localError,err,error,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("The distributed vectors do not have the same data type.",err,error,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vectors do not have the same library type.",err,error,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",err,error,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",err,error,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DistributedVector_VecDotIntg")
#endif
    RETURN
999 CALL ERRORS("DistributedVector_VecDotIntg",err,error)
#if DEBUG
    CALL EXITS("DistributedVector_VecDotIntg")
#endif
    RETURN 1
  END SUBROUTINE DistributedVector_VecDotIntg
  
  !
  !================================================================================================================================
  !

  !>Calculates the dot product of 2 distributed single-precision vectors on this computational node
  SUBROUTINE DistributedVector_VecDotSp(distributedVectorA,distributedVectorB,dotProduct,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorA !<A pointer to the distributed vector A
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorB !<A pointer to the distributed vector B
    REAL(SP), INTENT(OUT) :: dotProduct !<The dot product on this computational node
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string
    !Local Variables
    INTEGER(INTG) :: dataTypeA,dataTypeB,i
    TYPE(VARYING_STRING) :: localError

#if DEBUG
    CALL ENTERS("DistributedVector_VecDotSp",err,error,*999)
#endif

    IF(ASSOCIATED(distributedVectorA) .AND. ASSOCIATED(distributedVectorB)) THEN
      IF(distributedVectorA%VECTOR_FINISHED .AND. distributedVectorB%VECTOR_FINISHED) THEN
        IF (distributedVectorA%LIBRARY_TYPE==distributedVectorB%LIBRARY_TYPE) THEN
          CALL DistributedVector_DataTypeGet(distributedVectorA,dataTypeA,err,error,*999)
          CALL DistributedVector_DataTypeGet(distributedVectorB,dataTypeB,err,error,*999)
          IF(dataTypeA==dataTypeB) THEN
            SELECT CASE(distributedVectorA%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(distributedVectorA%CMISS)) THEN
                IF(distributedVectorA%CMISS%DATA_SIZE==distributedVectorB%CMISS%DATA_SIZE) THEN
                  IF(distributedVectorA%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
                    dotProduct=0.0_SP
                    DO i=1,distributedVectorA%CMISS%DATA_SIZE
                      dotProduct=dotProduct+(distributedVectorA%CMISS%DATA_SP(i)*distributedVectorB%CMISS%DATA_SP(i))
                    ENDDO !i
                  ELSE
                    CALL FLAG_ERROR("Input distributed vector data type does not match output.",err,error,*999)
                  ENDIF
                ELSE
                  CALL FLAG_ERROR("The distributed vectors do not have the same size.",err,error,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",err,error,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Distributed vector PETSC is double-precision, output scalar should be DP",err,error,*999)
            CASE DEFAULT
              localError="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(distributedVectorA%LIBRARY_TYPE,"*",err,error))//" is invalid."
              CALL FLAG_ERROR(localError,err,error,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("The distributed vectors do not have the same data type.",err,error,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vectors do not have the same library type.",err,error,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",err,error,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",err,error,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DistributedVector_VecDotSp")
#endif
    RETURN
999 CALL ERRORS("DistributedVector_VecDotSp",err,error)
#if DEBUG
    CALL EXITS("DistributedVector_VecDotSp")
#endif
    RETURN 1
  END SUBROUTINE DistributedVector_VecDotSp

  !
  !================================================================================================================================
  !

  !>Calculates the dot product of 2 distributed double-precision vectors on this computational node
  SUBROUTINE DistributedVector_VecDotDp(distributedVectorA,distributedVectorB,dotProduct,err,error,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorA !<A pointer to the distributed vector A
    TYPE(DISTRIBUTED_VECTOR_TYPE), INTENT(IN), POINTER :: distributedVectorB !<A pointer to the distributed vector B
    REAL(DP), INTENT(OUT) :: dotProduct !<The dot product on this computational node
    INTEGER(INTG), INTENT(OUT) :: err !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: error !<The error string
    !Local Variables
    INTEGER(INTG) :: dataTypeA,dataTypeB,i
    TYPE(VARYING_STRING) :: localError

#if DEBUG
    CALL ENTERS("DistributedVector_VecDotDp",err,error,*999)
#endif

    IF(ASSOCIATED(distributedVectorA) .AND. ASSOCIATED(distributedVectorB)) THEN
      IF(distributedVectorA%VECTOR_FINISHED .AND. distributedVectorB%VECTOR_FINISHED) THEN
        IF (distributedVectorA%LIBRARY_TYPE==distributedVectorB%LIBRARY_TYPE) THEN
          CALL DistributedVector_DataTypeGet(distributedVectorA,dataTypeA,err,error,*999)
          CALL DistributedVector_DataTypeGet(distributedVectorB,dataTypeB,err,error,*999)
          IF(dataTypeA==dataTypeB) THEN
            SELECT CASE(distributedVectorA%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(distributedVectorA%CMISS)) THEN
                IF(distributedVectorA%CMISS%DATA_SIZE==distributedVectorB%CMISS%DATA_SIZE) THEN
                  IF(distributedVectorA%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
                    dotProduct=0.0_DP
                    DO i=1,distributedVectorA%CMISS%DATA_SIZE
                      dotProduct=dotProduct+(distributedVectorA%CMISS%DATA_DP(i)*distributedVectorB%CMISS%DATA_DP(i))
                    ENDDO !i
                  ELSE
                    CALL FLAG_ERROR("Input distributed vector data type does not match output.",err,error,*999)
                  ENDIF
                ELSE
                  CALL FLAG_ERROR("The distributed vectors do not have the same size.",err,error,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",err,error,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(distributedVectorA%PETSC)) THEN
                CALL Petsc_VecDot(distributedVectorA%PETSC%VECTOR,distributedVectorB%PETSC%VECTOR, &
                  & dotProduct,err,error,*999)
              ELSE
                CALL FLAG_ERROR("Distributed vector PETSC is not associated.",err,error,*999)
              ENDIF
            CASE DEFAULT
              localError="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(distributedVectorA%LIBRARY_TYPE,"*",err,error))//" is invalid."
              CALL FLAG_ERROR(localError,err,error,*999)
            END SELECT
          ELSE
            CALL FLAG_ERROR("The distributed vectors do not have the same data type.",err,error,*999)
          ENDIF
        ELSE
          CALL FLAG_ERROR("The distributed vectors do not have the same library type.",err,error,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",err,error,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",err,error,*999)
    ENDIF

#if DEBUG
    CALL EXITS("DistributedVector_VecDotDp")
#endif
    RETURN
999 CALL ERRORS("DistributedVector_VecDotDp",err,error)
#if DEBUG
    CALL EXITS("DistributedVector_VecDotDp")
#endif
    RETURN 1
  END SUBROUTINE DistributedVector_VecDotDp
  
  !
  !================================================================================================================================
  !

  !>Adds values to a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_INTG(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to add
    INTEGER(INTG), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values added until dof mappings fixed. Ghost values that are added will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDICES(i))=DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDICES(i))+VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot add values for an integer PETSc distributed vector.",ERR,ERROR,*999)                    
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the integer data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indicies array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_INTG

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_INTG1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be added at
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value to be added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDEX)=DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDEX)+VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot add values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the integer data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_INTG1

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_SP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be added
    REAL(SP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to add
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDICES(i))=DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDICES(i))+VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot add values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
         ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the single precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_SP

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_SP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be added
    REAL(SP), INTENT(IN) :: VALUE !<The value to be added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDEX)=DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDEX)+VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot add values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the single precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_SP1

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_DP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be added
    REAL(DP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDICES(i))=DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDICES(i))+VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
                IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                  CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,SIZE(INDICES,1),DISTRIBUTED_VECTOR%PETSC% &
                    & GLOBAL_NUMBERS(INDICES),VALUES,PETSC_ADD_VALUES,ERR,ERROR,*999)
                ELSE
                  CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,SIZE(INDICES,1),DISTRIBUTED_VECTOR%PETSC% &
                    & GLOBAL_NUMBERS(INDICES),VALUES,PETSC_ADD_VALUES,ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the double precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_DP

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_DP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be added
    REAL(DP), INTENT(IN) :: VALUE !<The value to be added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    REAL(DP) :: PETSC_VALUE(1)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDEX)=DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDEX)+VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              PETSC_VALUE(1)=VALUE
              IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,1,DISTRIBUTED_VECTOR%PETSC%GLOBAL_NUMBERS(INDEX), &
                  & PETSC_VALUE,PETSC_ADD_VALUES,ERR,ERROR,*999)
              ELSE
                CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,1,DISTRIBUTED_VECTOR%PETSC%GLOBAL_NUMBERS(INDEX), &
                  & PETSC_VALUE,PETSC_ADD_VALUES,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT          
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the double precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_DP1

  !
  !================================================================================================================================
  !

  !>Adds values to a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_L(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be added
    LOGICAL, INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADDED_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_L(INDICES(i))=DISTRIBUTED_VECTOR%CMISS%DATA_L(INDICES(i)).OR.VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot add values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the logical data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_L

  !
  !================================================================================================================================
  !

  !>Adds one value to a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_L1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be added
    LOGICAL, INTENT(IN) :: VALUE !<The value to be added
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_ADD_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values to be added until dof mappings fixed. Ghost values that are added will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_L(INDEX)=DISTRIBUTED_VECTOR%CMISS%DATA_L(INDEX).OR.VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot add values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the logical data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_ADD_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_ADD_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_ADD_L1

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_INTG(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be get
    INTEGER(INTG), INTENT(OUT) :: VALUES(:) !<VALUES(i). On return, the value at the i'th specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    VALUES(i)=DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDICES(i))
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &

                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot set values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the integer data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indicies array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_INTG

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_INTG1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be get
    INTEGER(INTG), INTENT(OUT) :: VALUE !<On return, the value at the specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                VALUE=DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDEX)
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot set values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the integer data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_INTG1

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_SP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be get
    REAL(SP), INTENT(OUT) :: VALUES(:) !<VALUES(i). On return, the value at the i'th specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    VALUES(i)=DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDICES(i))
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the single precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_SP

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_SP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be get
    REAL(SP), INTENT(OUT) :: VALUE !<On return, the value at the specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                VALUE=DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDEX)
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot set values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the single precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_SP1

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_DP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be get
    REAL(DP), INTENT(OUT) :: VALUES(:) !<VALUES(i). On return, the value at the i'th specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i,PETSC_INDICES(SIZE(INDICES,1))
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    VALUES(i)=DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDICES(i))
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
                DO i=1,SIZE(INDICES,1)
                  PETSC_INDICES(i)=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(INDICES(i))-1 !PETSc uses global 0-based indices
                ENDDO !i
                IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                  CALL PETSC_VECGETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,SIZE(INDICES,1),PETSC_INDICES,VALUES, &
                    & ERR,ERROR,*999)
                ELSE
                  CALL PETSC_VECGETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,SIZE(INDICES,1),PETSC_INDICES,VALUES,ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the double precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_DP

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_DP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be get
    REAL(DP), INTENT(OUT) :: VALUE !<On return, the value at the specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: PETSC_INDEX(1)
    REAL(DP) :: PETSC_VALUE(1)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                VALUE=DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDEX)
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              PETSC_INDEX=DISTRIBUTED_VECTOR%DOMAIN_MAPPING%LOCAL_TO_GLOBAL_MAP(INDEX)-1 !PETSc uses global 0-based indices
              IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                CALL PETSC_VECGETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,1,PETSC_INDEX,PETSC_VALUE,ERR,ERROR,*999)
              ELSE
                CALL PETSC_VECGETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,1,PETSC_INDEX,PETSC_VALUE,ERR,ERROR,*999)
              ENDIF
              VALUE=PETSC_VALUE(1)
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the double precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_DP1

  !
  !================================================================================================================================
  !

  !>Gets values in a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_L(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be get
    LOGICAL, INTENT(OUT) :: VALUES(:) !<VALUES(i). On return, the value in the i'th specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    VALUES(i)=DISTRIBUTED_VECTOR%CMISS%DATA_L(INDICES(i))
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot set values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the logical data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_L

  !
  !================================================================================================================================
  !

  !>Gets one value in a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_L1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be get
    LOGICAL, INTENT(OUT) :: VALUE !<On return, the value in the specified index
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_GET_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
              VALUE=DISTRIBUTED_VECTOR%CMISS%DATA_L(INDEX)
            ELSE
              LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                & " is invalid. The index must be between 1 and "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the logical data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_GET_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_GET_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_GET_L1

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_INTG(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be set
    INTEGER(INTG), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_INTG",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDICES(i))=VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot set values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the integer data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indicies array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_INTG")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_INTG",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_INTG")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_INTG

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed integer vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_INTG1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be set
    INTEGER(INTG), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_INTG1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_INTG_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_INTG(INDEX)=VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot set values for an integer PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the integer data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_INTG1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_INTG1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_INTG1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_INTG1

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_SP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be set
    REAL(SP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_SP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDICES(i))=VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot get values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the single precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_SP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_SP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_SP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_SP

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed single precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_SP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be set
    REAL(SP), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_SP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_SP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_SP(INDEX)=VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            CALL FLAG_ERROR("Cannot set values for a single precision PETSc distributed vector.",ERR,ERROR,*999)          
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the single precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_SP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_SP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_SP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_SP1

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_DP(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be set
    REAL(DP), INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_DP",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDICES(i))=VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
                IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                  CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,SIZE(INDICES,1),DISTRIBUTED_VECTOR%PETSC% &
                    & GLOBAL_NUMBERS(INDICES),VALUES,PETSC_INSERT_VALUES,ERR,ERROR,*999)
                ELSE
                  CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,SIZE(INDICES,1),DISTRIBUTED_VECTOR%PETSC%GLOBAL_NUMBERS( &
                    & INDICES),VALUES,PETSC_INSERT_VALUES,ERR,ERROR,*999)
                ENDIF
              ELSE
                CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the double precision data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_DP")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_DP",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_DP")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_DP

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed double precision vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_DP1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be set
    REAL(DP), INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: PETSC_INDEX(1)
    REAL(DP) :: PETSC_VALUE(1)
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_DP1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_DP_TYPE) THEN
          SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
          CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
              !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
              IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                DISTRIBUTED_VECTOR%CMISS%DATA_DP(INDEX)=VALUE
              ELSE
                LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                  & " is invalid. The index must be between 1 and "// &
                  & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
            IF(ASSOCIATED(DISTRIBUTED_VECTOR%PETSC)) THEN
              PETSC_INDEX(1)=DISTRIBUTED_VECTOR%PETSC%GLOBAL_NUMBERS(INDEX)
              PETSC_VALUE(1)=VALUE
              IF(DISTRIBUTED_VECTOR%PETSC%USE_OVERRIDE_VECTOR) THEN
                CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%OVERRIDE_VECTOR,1,PETSC_INDEX,PETSC_VALUE,PETSC_INSERT_VALUES, &
                  & ERR,ERROR,*999)
              ELSE
                CALL PETSC_VECSETVALUES(DISTRIBUTED_VECTOR%PETSC%VECTOR,1,PETSC_INDEX,PETSC_VALUE,PETSC_INSERT_VALUES, &
                  & ERR,ERROR,*999)
              ENDIF
            ELSE
              CALL FLAG_ERROR("Distributed vector PETSc is not associated.",ERR,ERROR,*999)
            ENDIF
          CASE DEFAULT
            LOCAL_ERROR="The distributed vector library type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the double precision data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_DP1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_DP1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_DP1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_DP1

  !
  !================================================================================================================================
  !

  !>Sets values in a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_L(DISTRIBUTED_VECTOR,INDICES,VALUES,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDICES(:) !<INDICES(i). The i'th index to be set
    LOGICAL, INTENT(IN) :: VALUES(:) !<VALUES(i). The i'th value to set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    INTEGER(INTG) :: i
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_L",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(SIZE(INDICES,1)==SIZE(VALUES,1)) THEN
          IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
            SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
            CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
              IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
                DO i=1,SIZE(INDICES,1)
                  !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
                  IF(INDICES(i)>0.AND.INDICES(i)<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
                    DISTRIBUTED_VECTOR%CMISS%DATA_L(INDICES(i))=VALUES(i)
                  ELSE
                    LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDICES(i),"*",ERR,ERROR))// &
                      & " is invalid. The index must be between 1 and "// &
                      & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
                    CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
                  ENDIF
                ENDDO !i
              ELSE
                CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
              ENDIF
            CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
              CALL FLAG_ERROR("Cannot set values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
            CASE DEFAULT
              LOCAL_ERROR="The distributed vector library type of "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            END SELECT
          ELSE
            LOCAL_ERROR="The distributed data type of "// &
              & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
              & " does not correspond to the logical data type of the given values."
            CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
          ENDIF
        ELSE
          LOCAL_ERROR="The size of the indices array ("//TRIM(NUMBER_TO_VSTRING(SIZE(INDICES,1),"*",ERR,ERROR))// &
            & ") does not conform to the size of the values array ("//TRIM(NUMBER_TO_VSTRING(SIZE(VALUES,1),"*",ERR,ERROR))//")."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_L")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_L",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_L")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_L

  !
  !================================================================================================================================
  !

  !>Sets one value in a distributed logical vector.
  SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_L1(DISTRIBUTED_VECTOR,INDEX,VALUE,ERR,ERROR,*)

    !Argument variables
    TYPE(DISTRIBUTED_VECTOR_TYPE), POINTER :: DISTRIBUTED_VECTOR !<A pointer to the distributed vector
    INTEGER(INTG), INTENT(IN) :: INDEX !<The index to be set
    LOGICAL, INTENT(IN) :: VALUE !<The value to be set
    INTEGER(INTG), INTENT(OUT) :: ERR !<The error code
    TYPE(VARYING_STRING), INTENT(OUT) :: ERROR !<The error string
    !Local Variables
    TYPE(VARYING_STRING) :: LOCAL_ERROR
    
#if DEBUG
    CALL ENTERS("DISTRIBUTED_VECTOR_VALUES_SET_L1",ERR,ERROR,*999)
#endif

    IF(ASSOCIATED(DISTRIBUTED_VECTOR)) THEN
      IF(DISTRIBUTED_VECTOR%VECTOR_FINISHED) THEN
        IF(DISTRIBUTED_VECTOR%DATA_TYPE==MATRIX_VECTOR_L_TYPE) THEN
        SELECT CASE(DISTRIBUTED_VECTOR%LIBRARY_TYPE)
        CASE(DISTRIBUTED_MATRIX_VECTOR_CMISS_TYPE)
          IF(ASSOCIATED(DISTRIBUTED_VECTOR%CMISS)) THEN
            !Allow all values set until dof mappings fixed. Ghost values that are set will not be propogated
            IF(INDEX>0.AND.INDEX<=DISTRIBUTED_VECTOR%CMISS%DATA_SIZE) THEN
              DISTRIBUTED_VECTOR%CMISS%DATA_L(INDEX)=VALUE
            ELSE
              LOCAL_ERROR="Index "//TRIM(NUMBER_TO_VSTRING(INDEX,"*",ERR,ERROR))// &
                & " is invalid. The index must be between 1 and "// &
                & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%CMISS%DATA_SIZE,"*",ERR,ERROR))//"."
              CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
            ENDIF
          ELSE
            CALL FLAG_ERROR("Distributed vector CMISS is not associated.",ERR,ERROR,*999)
          ENDIF
        CASE(DISTRIBUTED_MATRIX_VECTOR_PETSC_TYPE)
          CALL FLAG_ERROR("Cannot set values for a logical PETSc distributed vector.",ERR,ERROR,*999)          
        CASE DEFAULT
          LOCAL_ERROR="The distributed vector library type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%LIBRARY_TYPE,"*",ERR,ERROR))//" is invalid."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        END SELECT
        ELSE
          LOCAL_ERROR="The distributed data type of "// &
            & TRIM(NUMBER_TO_VSTRING(DISTRIBUTED_VECTOR%DATA_TYPE,"*",ERR,ERROR))// &
            & " does not correspond to the logical data type of the given value."
          CALL FLAG_ERROR(LOCAL_ERROR,ERR,ERROR,*999)
        ENDIF
      ELSE
        CALL FLAG_ERROR("The distributed vector has not been finished.",ERR,ERROR,*999)
      ENDIF
    ELSE
      CALL FLAG_ERROR("Distributed vector is not associated.",ERR,ERROR,*999)
    ENDIF
    
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_L1")
#endif
    RETURN
999 CALL ERRORS("DISTRIBUTED_VECTOR_VALUES_SET_L1",ERR,ERROR)
#if DEBUG
    CALL EXITS("DISTRIBUTED_VECTOR_VALUES_SET_L1")
#endif
    RETURN 1
  END SUBROUTINE DISTRIBUTED_VECTOR_VALUES_SET_L1

  !
  !================================================================================================================================
  !
  
END MODULE DISTRIBUTED_MATRIX_VECTOR
